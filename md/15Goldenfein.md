---
Pr-id: MoneyLab
P-id: INC Reader
A-id: 10
Type: article
Book-type: anthology
Anthology item: article
Item-id: unique no.
Article-title: title of the article
Article-status: accepted
Author: name(s) of author(s)
Author-email:   corresponding address
Author-bio:  about the author
Abstract:   short description of the article (100 words)
Keywords:   50 keywords for search and indexing
Rights: CC BY-NC 4.0
...


# ‘Dropouts’: An Interview with Lilly Irani, Alex Hanna, Khadijah Abdurahman

### Interviewed by Jake Goldenfein

The participants in this interview connected through their collective
decision to withdraw from the ‘Power and Accountability in Tech’
conference, hosted by UCLA’s Institute for Technology, Law & Policy in
November 2021. Here, they reflect on the politics of ‘dropping out,’
connecting it to central themes of the edited volume. Recalling that the
analytical work of Economies of Virtue is to untangle the complex
dilemmas and paradoxes facing researchers in their interactions with
industrial capital, dropping out responds to another dimension of the
politics of industry money. Whereas many scholars seek out interaction
with industry through direct recruitment, funding support, or
presentation platforms, dropping out demonstrates a commitment to
delegitimizing the co-option of scholarly forums for building industrial
interests’ intellectual and reputational capital. While
industry-supported events often amplify congenial ideas, the
participation of critical scholars still provides intellectual
legitimacy. Critical voices may cloak problematic ideas in scholarly
livery, helping to construct a framing of even-handed analysis,
participant diversity, and intellectual rigour. The labour of critical
scholars thus becomes a tool for industrial interests to leverage
academic authority into policy that supports their agenda. Of course,
the politics of the forum are critical, and not all industrial money is
equal. So when is it appropriate to engage, to call-out, or to drop out?

Jake Goldenfein (JG): Let’s start with introductions.

Lilly Irani (LI): My name is Lilly Irani. I’m an Associate Professor at
UC San Diego, and I’ve been organizing with Amazon Mechanical Turk
workers for a few years, trying to improve their work conditions. For a
long time before, I ran a software system with Six Silberman for workers
on the Amazon Mechanical Turk platform. I also organize a Tech Workers’
Coalition. Before I went into academia, in my twenties, I worked at
Google in the Church of Make the World a Better Place, in what I didn’t
really realize then was the tentacles of a digital infrastructural
empire. If Tech Workers’ Coalition had existed, maybe I would have
stayed and organized, but organizing was not at all part of my
self-understanding or social knowledge in my 20s.

Alex Hanna (AH): I’m Alex Hanna, and I’m a Research Scientist on the
Ethical AI Team at Google.[^15Goldenfein_1] I mostly study how data and machine
learning exacerbate racial, gender, and class inequalities. I don’t
quite have an organizing home right now, but in the past, I’ve done
labour organizing, organizing for trans healthcare, and campaign-based
things in the ‘tech ethics’ space.

J. Khadijah Abdurahman (KA): I’m Khadijah Abdurahman. Who am I
professionally? I would say that I research predictive analytics in the
child welfare system, which frankly, nobody cares about most of the
time. I don’t think people in tech think about child welfare or family
policing whatsoever. I do a lot of organizing with academics, less on
labour rights, and in certain ways, I’m trying to shift the discourse,
I’m even trying to shift how people think about ‘what is a discourse?’

JG: In the lead-up to the UCLA conference, Khadijah contacted me to
point out that this specific UCLA Institute was funded to the tune of
\$4 million by the Charles Koch Foundation. A web search suggests that
other philanthropies, like Brad Jones, the Troesh Family Foundation, and
Anthony and Jeanne Pritzker Family Foundation also contributed, giving
the institute a total endowment of something like \$10 million.
(Strangely, the grant documentation from the Charles Koch Foundation is
no longer available online; it certainly was available at the time of
the conference, but it was not available in more recent searches).

When Khadijah contacted me, I was somewhat unsure of how to proceed. I’m a relatively unknown scholar based on the other side of the world. Koch Foundation politics is not something that I innately understood. And so, I was worried my withdrawal would likely be meaningless. I wondered whether it would be better to use the time allocated to me in my panel to talk about Koch and industry funding. But once both Lilly and Alex had taken leadership on the issue, it made much more sense to withdraw as a gesture of solidarity to both of you.

Perhaps each of you could briefly explain how you came to initially be a participant in, and then ultimately a drop-out from, this event.

LI: I came to participate in the same way that I have been asked to
participate in what seems like infinite workshops about fixing
*something* with digital something over the last ten years. The Director
of the Institute at UCLA reached out to me asking me to talk about tech
worker activism. I said, ‘Well, shouldn’t you ask Meredith Whittaker,
because she actually was at Google organizing against Project
Maven[^15Goldenfein_2]’—something I was involved with in solidarity as an academic
doing a petition. And he said, ‘Oh yes, I already asked Meredith, and
she actually couldn’t do it, which is why I’m asking you.’ I said, ‘Oh,
that’s great. We’re on the same page.’ Oh, how wrong I was. After I’d
accepted, Khadijah reached out to me noting that this event had Koch
funding. I ended up talking to Khadijah and Meredith, and Meredith said,
‘Oh, I actually knew that, and I refused to participate on that basis,’
which I didn’t know.

For me, it was kind of a no-brainer to drop out, and in part. It was not only a function of my job security. It was also a function of having gone through a decade of what Meredith once called the ‘critical AI industrial complex’, where there’s funding of all these events to talk about ethical issues, and tons of foundations, governmental organizations, universities wanting to put themselves on the map, get grants, and maybe influence the discourse. I’ve been to meeting after meeting like these over the last ten years on digital labour issues because of the work I was doing with Amazon Mechanical Turk workers. And I eventually reached the conclusion that those fora don’t make a damned difference. In some ways, actually networking with researchers and lawyers and policy people who work at these companies is a drain on energy that should actually be spent building solidarities and organizing with people who are most directly affected, and have the most to say about how these systems should change.

So one, these things can be a drain, and two, they can start to contain people who are hoping to change these platforms but don’t have a plan. And so I thought the three hours that I was going to spend at this panel, and stressing out about what to say in this panel, could be better spent organizing with Turkers, or explaining to people about Koch funding, and joining and amplifying voices like Khadijah’s.

Life is limited; we only have so many hours in a day, and I’ve decided to be way more selective about who I give my hours to, for what solidarities.

JG: Thank you for this hour; I appreciate it.

AH: I became involved with this workshop in the same kind of way. There
was an invite. I had never heard of this centre. I asked somebody else
at UCLA what the centre was about; they seemed to vouch for them. I
don’t necessarily want to call out who that person is. And then in the
same way, I thought, ‘Okay, might as well.’ Khadijah then similarly
reached out to me after the event was advertised on Twitter and alerted
me to the Koch money. I was already kind of suspicious of the panel
because it was framed around holding companies to account internally,
and the people associated with it were from BSR \[Business for Social
Responsibility\] and corporate social responsibility. That was not
really a conversation that I was really interested in having because of
its pretty clear limits.

 Once Khadijah alerted me to that, I decided, ‘Okay, yeah, I’m backing out of this,’ and I stepped out of the event. I completely agree with Lilly that these things have their own economy—there’s an ‘economy of prestige’ that is very much bound up in these same conversations around being *the* whistle-blower or being *the* person holding tech to account. But there are some pretty clear limits to doing that from a pedestal. And putting work into that without understanding the dynamic takes the air out of the room for people that have much deeper analyses based on being at the receiving end of harms—people who are organizing; people who are subject to disproportionate surveillance and carcerality.

KA: I wasn’t invited to speak at this event. Also, I was thinking I
often don’t get invited to speak at different events. I don’t think
necessarily it’s because of where I sit in the academic hierarchy, but
because I spend a lot of time being like, ‘What the fuck are you doing?’
I was actually just laughing because Alex and I were supposed to be on
another panel in the wake of Timnit Gebru[^15Goldenfein_3] being fired, or
‘resignated’, or whatever the term is, that was also supposed to be on
tech worker organizing. Initially they only told me Alex was going to be
on the panel, and I thought, ‘Yeah, sure. I’m down.’ Then I learned that
someone who not only wasn’t a tech worker organizer, but an ardent white
supremacist with a lot of prestige, was also going to be on the panel.
So I said, ‘Absolutely not. I will not participate in this, because it
is a complete sham.’ Since the event was at a public university, they’re
not allowed to publicly disinvite people. So they had to cancel the
whole event, which was fine for me because why am I selling out for
this? For me, I don’t think that there’s this place that you can arrive
at where you’re not morally implicated whatsoever. But to the degree I
have agency, why should I be on this panel with a cartoon villain for no
reason whatsoever? I do have a reputation for this, and it’s pretty
widely known, so that probably results in me not being invited to a lot
of things.

With respect to the UCLA event, I had followed a piece that Kate Klonick published in The *New Yorker* about the Facebook Oversight Board, and I remember her being heavily criticized due to the length of time she spent embedded within Facebook. This was around the time that Jill Biden, the current President’s wife, had been making a big deal of being called ‘Dr Biden,’ and people were making that analogy to say Kate Klonick is being unfairly targeted as a woman, i.e. this is misogyny, and she is being held to different standards. And I had thought, no, I’m pretty sure it’s because she sold out to Facebook, and is peddling the corporate version of regime propaganda. Then I saw some tweets from David Golumbia, and I was able to figure out that Kate accepted a leadership position at Charles Koch Foundation Technology & Innovation. That to me was shocking, because again, we’re all implicated into these filthy worlds of no morality and terrible money. But Koch is very specific. Koch is so systematic in discussions around Critical Race Theory, and pumping poison into the environment, and they have coined the process through which academic research is evacuated of any and all meaning. They systematically use funding in order to create a deregulated policy space in which capital can be further monopolized.[^15Goldenfein_4] For Klonick to take the Koch money really stood out to me.

I then saw that Kate Klonick was scheduled to speak at the UCLA Law and Policy Institute’s conferences on Platform Accountability and discovered that the event and institute are funded by Koch. I reached out to you guys. And I will say, I’m not going to name the specific people, but because we all operate on these networks of proximity (even for me, I’m affiliated with UCLA too), I knew people who said no—who refused to step down. And we \[Khadijah and Jake\] had a conversation about whether it is just virtue signalling to step down. And to me, I feel like that’s a dangerous idea. I don’t think every publicly made moral decision is a performance. I do think that what we do in the world matters. On the one hand, I might not feel the same way about somebody doing an event at Microsoft at this very moment. But I think that there are these moments when we have greater agency. For instance, when people were walking out at Google because of Project Maven, or the stuff around Timnit. Sometimes there’s an opening for there to be a sea change.

AH: With the UCLA event, I was thinking, ‘Yeah, there’s a lot of friends
of ours that do this stuff.’ People we would actually consider allies or
friends. There’s a tension here, and you really have to think about what
the coalition looks like when people are taking money from certain
places. I think Koch is a very piqued case, but I’m also thinking about
the kind of conversations we were having after Google fired Timnit and
we started asking people to stop taking Google money. Some people did,
Queer in AI did, Black in AI did, I think Widening NLP (WiNLP) did. But
then some people came back to me and said, ‘Okay, we get this pittance
from Google. That’s going to put our programming at a disadvantage.’

I want to pull on this tension. There’s no hard line here, but it is important to ask what does this tension do? I want to talk about these acts of refusal or pulling out, and what that does at particular moments. I think there’s an analysis to be done there because it’s not always clear to me when the impact of withdrawing or pulling out is going to be the most effective. I’d love to get your thoughts on that.

KA: I was thinking about how funding organizes community, and we haven’t
released this yet, but I did an interview with Safiya Noble and Meredith
Whittaker about transformative justice when you’re running centres of
knowledge production.[^15Goldenfein_5] One of the comments that Meredith made stuck
with me: ‘What is a community?’ This is now a very Facebook-associated
word, but a lot of times the community—this large umbrella of critical
tech—is just whoever Aspen Institute, or whatever set of funders, have
brought together. And one person in any group might not just be
receiving Koch money, but might actually be supportive of Koch and
libertarianism and all of that. Another person might centre incarcerated
black women. A lot of what gets trafficked as community, or as
friendships or proximity, is very artificially produced. That becomes
even more mystifying during the pandemic when for the average person,
the actual terms and conditions of these relationships that are
presented in the form of an event, or a publication, or a ‘discourse,’
are not really visible. This is especially when discourses and
disciplines are often created by institutions and are not necessarily
accountable to us.

JG: In our \[Jake and Khadijah’s\] conversation you made a very
compelling point, which was that we have not reached normalisation of
Koch foundation money in the tech policy / AI ethics space. It wasn’t
common yet, and it didn’t have to be inevitable.

KA: In this specific case, it felt like there was an opening for a sea
change around Koch money and academic accountability, especially because
Koch’s hegemony in this particular domain hasn’t been cemented. On top
of that, the contents of the event were a little bit
questionable—somehow we’re going to be holding platforms accountable
through a panel?

So in this Alice in Wonderland rabbit hole, it did feel like this was a place where we could push, and it did feel uncomfortable, especially because I’m friends with some people that I was upset didn’t step down. They’re probably annoyed, like who am I, the academic nanny police shaming people for doing evil panels? But I do think we have to create some bright lines around what we will and will not do. It feels hard because a lot of how power operates is concealed, or is nebulous given our collective complicities. Additionally, we are in a pandemic, we’re all stressed the fuck out; where do we compromise, and where do we push, and where do we demand that others also step up?

LI: There’s this idea that there’s too many funders, and it’s
overwhelming, and it’s impossible. But it’s not that when you draw one
bright line, that’s the bright line forever. Because to me personally,
the bright line is capitalism. The question is whether saying no to Koch
gets me closer to ending capitalism, to be honest. But each line drawing
forces people to have the conversations about why this line is here, and
what other lines may be needed in the future. It’s not only about
performing withdrawal, it’s also about creating the conditions in which
more people can build their education, build their analysis, get
organized, and recognize the power we do have in withdrawing our labour
or other forms of direct action.

I agree with Khadijah that Koch is pretty unique in all the ways that she brilliantly articulated, and I also feel the need to take that line to other places. But if we can’t even agree on Koch, then we’re not going to get to those other more subtle, more friendly, less well-researched kinds of places—bright lines that we’ll need to draw in the future.

JG: Can we talk more about particular ‘moments’ or particular bright
lines? Is Koch so egregious compared to everyone else that we’re taking
this money from?

KA: There’s a tension! I really liked your paper on ‘Economies of
Virtue’[^15Goldenfein_6], in part because it raised the issue of Minderoo. I’m very
aware that I receive Minderoo funding, though somewhat indirectly
through AI Now. I’d never heard of Minderoo because I didn’t grow up in
Australia. But the shit is super evil. It’s based on Aboriginal land
dispossession, Twiggy Forrest is a sixth-generation Australian
colonizer, and specifically a champion of algorithmic allocation for
welfare benefits, which overlaps with my work. Minderoo is not visible
to Americans as an evil entity in the same way I’d imagine it is in
Australia. I am aware that I am getting something that is evil, but I’m
also operating in a situation of profound resource scarcity. We’re in
the Great Depression here, and I don’t just say for myself personally,
but I’m working with people who I’m constantly trying to funnel money
to, who I don’t want to ask to do things for free, and I feel
responsible to find them money. That for me, is part of the tension.

In terms of flagging bright lines, one of them for me is around the police. I will sit here with you, Alex, and I might have a critique of Google, but if you were working for ICE \[Immigration and Customs Enforcement\], we wouldn’t be on this Zoom call. And I’m sure vice versa. This is also more complex because we know that Google is entangled with the Department of Defence. I’m bringing this up specifically because Lilly and I were involved in this letter-writing to the FAccT[^15Goldenfein_7] central committee (central committee is so Marxist–Leninist, but whatever it is called), asking them to come out and take a position about the police. ‘Will you or will you not support the police and police research?’ And there was a refusal to draw this bright line, and that’s something very clear for me.

JG: I remember the 2019 Privacy Law Scholars Conference (PLSC) and its
relationship with Palantir. I recall there was an open letter requesting
the conference remove Palantir as a sponsor, and the conference
organizers responded by saying, ‘Well, you know…we’ve got a sponsorship
policy. It says they’re not allowed to have any influence, so we’d
rather have them at the table.’ In the end, there was sufficient
concerted academic pressure to dump them, but that happened *in spite
of* the conference sponsorship policy, rather than because of it.

I think that response represents a broader problem of the depoliticization of these scholarly spaces. It’s all become quite procedural, and you see it in conference sponsorship policies that are like, ‘As long as the sponsor has no influence over the content or agenda of the event or the institute, then everything’s fine.’ And that evacuates the capacity to have bright lines that we won’t cross, or even to think about them, because there’s no space for taking an ethical position or developing your subjectivity as a researcher, according to whatever ethical stance you have in relation to that.

LI: When you mentioned the depoliticization of the university, Jake, the
issue of academic freedom kept coming up in my head, because there are
related things happening at my university right now.

One of the ways the Koch brothers defend what they do in universities is to call it ‘academic freedom.’ And academic freedom has now become about protecting the space for racist, misogynistic, fascist speech. I recall reading that in the 1930s and ’40s in the United States, academic freedom was something that universities gave to faculty and graduate students, basically university workers that were unionizing under a socialist banner. Then I think the AAUP \[American Association of University Professors\] accepted a weak version of academic freedom and tenure in place of more substantive collective control over the university.[^15Goldenfein_8] Academic freedom was thus held up as this ideal suggesting, ‘You should be an individual. You should always have fidelity to what you think is your truth’. But any kind of sociological analysis around ‘who is this community that’s been constructed for you,’ or ‘what’s the history of your discipline,’ are then rendered as things that threaten individual scientific freedom. And I feel like the Koch Brothers traffic in that.

KA: Related to this point about moral evacuation and depoliticization is
that an announcement of power is not unexaminable, but it is often
forcefully concealed. In part, this is because of competing political
projects that mediate the way funding happens. I wrote about one of
these examples in a *Medium* blog post called ‘Encoding Hindutva:
Shalini Kantayya and Coded Bias’[^15Goldenfein_9] describing how the producer of
Coded Bias had been the poster girl for the Hindu American Foundation,
which has ties to the paramilitary organisation RSS, and Narendra Modi.
The thing is, for 90 percent of the people in that movie, I don’t think
that they were like, ‘Yay, Hindutva,’ I think that they literally had no
idea. They were asked to be in a movie that was generically against Big
Tech, and they agreed. But once we begin to understand that 80 percent
of H1B visas in the US go to India, and the majority of those go to
Brahmins, we can see how that casteism is reproduced in Silicon Valley.
Even one of the supervisors involved in Timnit’s firing was Brahmin. And
this is associated with a kind of anti-blackness. Various fascistic
political projects are connected to this funding sphere. But we all
can’t know everything, and we don’t all have the social context for
everything. I know a little bit more about Hindutva, I know a little bit
more about black American politics here compared to say, what’s
happening in Brazil with Bolsonaro. But this is some of what obfuscates
the power dynamics of what’s happening. A lot of these decisions happen
in closed rooms, and understanding who ends up in those rooms is a very
long historical project.

AH: I’m thinking about these comments on the depoliticization of
academia alongside Khadijah’s foreword to the *Logic* issue that
mentions a letter that the Manhattan Project scientists sent to Truman
pleading not to drop the atomic bomb.[^15Goldenfein_10] AI scientists have found
themselves in a sort of hapless political space. I listened to Geoff
Hinton, who’s one of the godfathers of machine learning or AI give a
talk internally at Google. AI, or deep learning, had its resurgence
because they happened to keep on jamming on this one type of technique
since the ’80s, and then they eventually had so much data that it
actually worked. But then these people, the research scientists, all of
a sudden had just immense amounts of power and were not sure what to do
with it. Hinton recalled how they were getting huge offers after they
won the ImageNet competition. He recounted how they were at NeurIPS one
year, it was in a casino, and they were getting offers of millions and
millions of dollars for the company that he had started with Alex
Krizhevsky and Ilya Sutskever (who went on to co-found OpenAI).

These people didn’t have a political analysis prior to that moment. And then they developed this thing that’s the talk of the town, they’re having millions, if not billions of dollars thrown at them, and this means they have less incentive to develop that kind of analysis.

But that’s a weird idiosyncrasy of computer scientists. I’m a sociologist, but the way one is trained even in sociology, a discipline which ostensibly has something to do with analysing inequality, actually has very little to do with ameliorating inequality. At the big annual meetings of sociologists this perennial thing happens that’s just so divorced from reality—the questioning of whether you should be engaged in some kind of politics. At worst, many people within the discipline are just outright right-wing white supremacists. But then, there’s also a lot of these good liberals who are just really glad that Trump has gone, and now we just have to get back to doing what we were doing when Obama was in office.

So it’s strange being in this space now, because these events are so entirely inconsequential. The process by which these people were simply good scientists, but then thrown into positions of power, means there’s just more incentive to denude these things of any kind of political effect in the world.

KA: The Manhattan Project scientists, for all intents and purposes,
didn’t think of themselves as political actors. I think part of what I
was getting at in that *Logic* editorial is my little joke about how
techno-capitalism puts in charge of humanity people that didn’t take the
humanities. And so depoliticization is this excision of the humanities.
At the same time, and Frank Pasquale makes this argument in the book
about robots that’s not about robots—‘New Laws of Robotics’[^15Goldenfein_11]—there’s
this idea that AI is displacing human expertise. The danger is that Zoom
will collapse the middleman, we’ll have no universities, etc. That means
we need to rebuild our institutions, civic society, and the state, which
have each been financialized, fragmented, and broken away as new money
has concentrated into these new forms of capital.

But the reason that I specifically called in the black studies people
\[in that editorial\], and tried to take a different orientation from calling out the morally evacuated people that drive me nuts, is because I know that they know more than me. The thing that black studies scholars particularly know more than me about is what it means to be human, and what the problems are with Enlightenment thinking. Because when I think about the moral bright lines, I think about Montesquieu saying, ‘No more torture,’ and, ‘How are we going to organize these different polities?’ But at the same time, that Enlightenment rationalism, the idea of ‘just being a scientist’ is predicated on colonialism and mass dispossession. It’s predicated not just on IBM finetuning the transportation of Jews during the Holocaust, but also the Herero and Nama genocide that predated it.

So for me, the issue is this need to bring in social context; we need to bring in the humanities. But at the very same time, we’re all *in a context*, and we’re thinking with tools through which people have already been excluded from this category of human. Even our own imagination is foreclosed by that. And I don’t feel like I have all the tools to figure that out, but I do know that it cannot just be individual. I think we’re in trouble if we just go with the individual route. But I also don’t know exactly what it means to be collective. I believe we need this labour organizing, I believe we need to be intentional, but we still operate in these institutions. I don’t think the university is going away tomorrow. I also don’t think Google is going away. So how do we operate on this structural level?

AH: This is very connected to questions of organization and
manifestations of political leftism. Khadijah, I’m thinking about a
thread you posted \[on Twitter\] maybe a few months ago that said
something about some of your initial political homes being with the
Revolutionary Communist Party. I have some sympathies for that; my first
political home when I was 19 was ‘News and Letters’, which was a small
Trotskyist sect run by Raya Dunayevskaya and C.L.R. James before they
parted ways. They had this newspaper, which now you see young men with
‘newsie caps’ peddle on college campuses. But they did have these
worker–thinkers really contributing to those magazines, and they were
actually quite effective mediums for political organizing. It’s part of
a larger question about what political organizing looks like, and it’s
just such a fragmented sort of space right now. So much ink has been
spilled on the state of the leftist organization. This idea of labour
organization, or political organization, in this mass platformed era is
not easy to crack.

In an essay by Dean Spade in *Social Text*,[^15Goldenfein_12] he breaks down several activism strategies. The first is marching and overt activism, but then there’s also these architectures of care which frame up a discussion of the importance of mutual aid. Much of his larger book[^15Goldenfein_13] on mutual aid is about ‘how to have a meeting,’ and how to get people to commit to doing things; like, how to use sign-up sheets. I thought this was fantastic because it’s a good prompt to revisit and think about what these networks look like if you start doing the 101.

JG: Alex, I think you’ve highlighted this real tension between some the
deeply individual imperatives that are a big part of what you earlier
called an ‘economy of prestige’—being the person that receives the kudos
for calling out the bad corporate actor and getting the speaking
engagements and academic kudos, etc.—and the collective imperatives of
political organizing, which is not rewarded at all in the same way. And
this inevitably has to do with how our institutional lives are
organized.

LI: I don’t know if by ‘institutional’ you mean the way the university
is set up—is that what you're thinking about? I guess I have extreme
ambivalence about the university, learned both from reading Moten and
Harney about the Undercommons and Fugitive Planning[^15Goldenfein_14] where they
argue that the university is a site for making up governmental knowledge
in some way; and also from the undocumented undergrads I teach who are
coming to this space, and having a space away from where they grew up
where they can encounter new people and new ideas, and become something
that they want to become.

And so I guess I think of the university as my employer—do I try to reform my employer? Or do I try to organize to ultimately have some kind of substantive democratic ownership over my employer in the long term, and then the reforms that I bother fighting for are the ones that help us conceptualize what that would even be, as well as to build the skills and relationships that we need to get there?

Also, and I was thinking about this while Khadijah was talking about this question ‘what is the human?’, I totally understand the ways in which you mean that black studies can speak to that question, but at the end of the day I also don’t cede that question to any academics. I feel informed by what black studies can teach us, and informed by what others can teach, but we have to come up with the answer to that through our own organizing too. Maybe I’m idealistic about organizing, but I feel like it can be done in a way that draws out the best parts of the kinds of commitments that people can develop when they’re challenged by having to figure out how to live together.

KA: I hear your ambivalence towards the academy. I think part of what I
was trying to get at with the *Logic* editorial is that the Audre Lorde
quote—this idea of you can’t dismantle the master’s house with the
master’s tools—is very layered. The academy does not own knowledge
production. And so for me, when I’m thinking about black studies, I
don’t even necessarily mean this particular formatting through text that
we disseminate in this almost liturgical way through academic lectures.
It can exist in so many ways.

There’s another tension around funding *through* universities. As someone who’s permanently on the periphery, my relationship to the university is also informed by my duty to people. Because for me, some centralisation is required. Decentralization is very in vogue now, but there’s this question of how am I getting resources to people? People who receive funding can become funnels, and they hopefully aspire to build this thing that they didn’t have. When I wrote the ‘Moral Collapse of AI Ethics,’[^15Goldenfein_15] Safiya Noble joked, ‘Do you have health insurance?’ And I said, ‘No, son, I’m poor, like actually poor.’ And they hooked me up. I get funding from them, and you know what, they leave me alone. No one that I get funding from, Columbia, NYU, UCLA, ever asks me what the hell I’m doing, ever. No one checks in on me, I have no deadlines, I have no mandated meetings, I’m never on Zoom, ever. No one cares what I’m doing. The other aspect of this is that eventually I have to go get new funding for 2023. But I’m saying this is what people can create for other people through the university. And I feel like that’s a real duty of care that Meredith and Safiya showed—and not only to me.

I also feel like I have academic freedom, to the degree I’d use that term. I feel like I can come up with a project and pitch it to people. For instance, I can critique Ben \[Tarnoff\] and Moira \[Weigel\], but they let me do whatever I wanted with *Logic*; they really didn’t interfere. These opportunities exist. I think Minderoo funds the Incubator at UCLA, and I received a social impact grant, I think in total of \$50,000. I was able to fund six projects globally, six people in Kashmir, one in Oromia, and I also asked nothing of them. I said, ‘Do whatever you want with this,’ and, ‘If I can help you, I’ll help you, but you don’t have to report anything.’ For the *Logic* issue, they normally pay people \$250–\$500 for an essay, but we gave people \$2,000 to \$4,000. My idea was to pump and infuse cash to all the people doing dope work but are broke all the time. \$4,000 is not life-altering, but it gives people something. This is just to give you some concrete data points on what funding can look like. At the same time there’s also evil things involved, but I don’t know how to deal with that?

AH: There’s a few things I want to pull on. When I was talking about the
‘economy of prestige’, and this idea that there are certain people that
one must ‘recruit’ for talks, I’m not saying that these people are wrong
for taking the speaking engagements. I’ve done plenty of that myself.
It’s more that incentives in the academic world are very much optimized
around doing talks and publishing, etc. There are these worst cases,
where there are people who are very public, but are well known in the
whisper networks to be abusers, siphoning off all kinds of labour from
junior people, while using language of liberation, or AI ethics or
whatever—you pick your own vocabulary. And that isn’t in service of, as
you put it Khadijah, a duty to people. And what it means to have a duty
to X, or a duty to people, is one analytic that I’m continually coming
back to. I say duty to X because, depending on where you sit, that X is
a variable. At Google for instance, it’s a duty to the user (that’s the
way it’s framed; people become users). In the non-profits, it’s a duty
to funders, or whoever. This is a long way of saying that a person’s
subject position is really important, and it takes so much to do the
work of resisting those incentives, just because everyone else at your
institution is just breathing down your neck.

I’ve started reading Sara Ahmed’s *Complaint!*.[^15Goldenfein_16] One of the things she talks about is how lodging complaints in the institution really illustrates what those incentives are, and what the inner workings of the institution are. There’s really no academic analysis yet about how people get swept along in these tides of prestige and publication.

LI: I went into academia because I felt like there wasn’t actually room
in the tech sector for work that aligned with my values, and I didn’t
know that that was just a feature of capitalist alienation. So, another
thing that you get swept along with in academia is this hope that, ‘Oh,
I want to do critical work. I want to be able to say things that I can’t
say at a company.’ But to what end? That ‘saying’ doesn’t necessarily
lead to anything either. I suppose I’m suggesting that some people are
chasing prestige, and some people are chasing an idealistic hope, but in
both cases capital wins.

JG: Well, speaking of idealism in service of capital, you mentioned
earlier writing letters to FAccT organizing committees. FAccT seems to
occupy this important institutional position in the AI ethics / tech
policy space because of its capacity to define what gets taken up as an
agenda in the ‘fairness’ field, and to a degree, AI Ethics more broadly,
as well as its capacity to generate networks, and facilitate
industry–academy interaction. It seems to me that telling FAccT to take
a stand could be a very meaningful action. Where did the impetus towards
that action come from?

LI: I may delete all of what I’m about to say.

The impetus for the FAccT letter came from one attendee getting really angry at the town hall about Christo Wilson’s paper that basically let a company, ‘pymetrics,’ define the terms of fairness auditing its own hiring algorithm.[^15Goldenfein_17] That paper was then trotted around by the company as an example of being independently audited so that local city councils and regulators would consider this algorithm ready to go. The process of working on that letter then expanded into discussion of other issues. Khadijah was involved in the beginning. Alex was also involved in those conversations.

Asking FAccT to take a stand, or seeing that they won’t take a stand publicly, is a meaningful action. I’m curious what the others think about this. As with a lot of organizing, it’s learning by doing, and so there were a lot of conversations between me and Alex, me and Khadijah, and this network of people who signed the letter, and some of the people who didn’t sign it but were glad it was happening. There were conversations asking, ‘Yeah, is there a point in making a demand? Is there a point to making a demand about *this* particular thing?’ It forced us to clarify, to some extent, our analysis of what the institution was in a way that could maybe be made public.

But to be honest, I also just wish that we could have a group meeting to talk these things out because it felt like it took so long, so many one-on-one, back-channel conversations. Issues of trust and time are so intense.

KA: The funny thing about FAccT is that I’ve never been to FAccT. I’ve
been hate-attending on Twitter for so long.

For me, the letter very much encapsulated the ‘who is our “community”’ question. Because the chasm between the positionalities of people on there, between the ‘Fuck the police, ACAB’ people, and the ‘I work for them’ people, to me, felt unresolvable. But the thing that stood out to me is FAccT’s organizing capacity. Institutionally, in person, it is impossible for us to replicate that with any kind of network of ourselves. Digitally, the hegemony that they maintained, for me represented the level of ambivalence, and why it’s so hard to organize academics. Because fine, people are burnt out, and administratively I think it is somewhat difficult to organize a conference. But I just really struggled. I felt like there’s a point at which you’re not going to push those type of people forward, and they’ll just refuse. Particularly, because one of the many reasons that I hated FAccT was that when I reached out it became clear that one of the people on the Executive Committee was one of the developers of the Alleghany County Family Screening Tool, and to me this was just beyond.

So I asked, ‘Why don’t we have our own conference?’ The whole thing is based on free labour, but a conference specifically feels like something that we could actually pull off. Even if you did three workshops and you called it, I don’t know, Alternate FAccT Conference, or the Ex-FAccT Drop-Outs Conference. But the motivation to do that…? I feel like if we mandated that people do it, they would do it. But the idea that there would be a groundswell just seems so outside of the collective imagination.

AH: One question I ask is why even organize around FAccT? And this is
something that Lilly and I have had a lot of conversations about. These
people are just going to just hunker down. But maybe it is worth forcing
them to show their hand?

In the end, Lilly did a good job convincing me, but also I just feel like this is becoming a continual conversation around FAccT. I went to the first in-person FAccT in New York in 2018, and I remember someone standing up and they were just like, ‘What the fuck is this? Why do we care?’ They were talking about how all these conversations are basically immaterial, saying ‘You’re just thinking about fair ways to surveil people.’

LI: One thing about the FAccT letter is that the conversations around it
helped clarify, through defining what’s wrong with FAccT and having
people sign on to saying publicly that this thing is wrong, that if we
were going to start another conference like Khadijah’s talking about, we
would need to be clear on what some of its commitments would have to be.

The other thought that just keeps swirling around in my head is that it becomes really clear that FAccT’s commitment is to maintaining a big tent or ‘maintaining a table,’ which is how they keep saying it. That’s the term they used with PLSC too. But this reflects some fantasy that through the free exchange of ideas the right thing is going to happen.

JG: I appreciate the nuance that everyone has brought to this
conversation. Dropping out is a difficult prospect, but you’ve really
shown its relationship to the complexities of academic life. Researchers
and scholars are constantly negotiating their relationships to
coalitions and communities, the politics of money and prestige,
institutional imperatives and academic freedom, and who we ultimately
owe duties to. I can’t thank all you enough for really reminding us all
of what’s at stake.

## Works Cited

Abdurahman, J. Khadijah. ‘On the Moral Collapse of AI Ethics’ *Medium*
blog,8 December 2020,
https://upfromthecracks.medium.com/on-the-moral-collapse-of-ai-ethics-791cbc7df872.

——. ‘Encoding Hindutva: Shalini Kantayya and Coded Bias’, *Medium* blog,
2 June 2021,
https://upfromthecracks.medium.com/encoding-hindutva-shalini-kantayya-and-coded-bias-721fe04f225f.

——. ‘Holding to Account: Safiya Umoja Noble and Meredith Whittaker on
Duties of Care and Resistance to Big Tech’, *Logic: Beacons* 15 (2022),
https://logicmag.io/beacons/holding-to-account-safiya-umoja-noble-and-meredith-whittaker/.

——. ‘A Body of Work That Cannot be Ignored’, *Logic: Beacons* 15 (2022),
https://logicmag.io/beacons/a-body-of-work-that-cannot-be-ignored/.

Ahmed, Sara. *Complaint!*, Durham: Duke University Press, 2021.

Dickenson, Tim. ‘Inside the Koch Brothers Toxic Empire’, *Rolling
Stone*, 24 September 2014,https://www.rollingstone.com/politics/politics-news/inside-the-koch-brothers-toxic-empire-164403/.

Harney, Stefano and Moten, Fred. *The Undercommons: Fugitive Planning &
Black Study*, London: Minor Compositions, 2013.

Meyerhff, Eli, Johnson, Elizabeth, Braun, Bruce. ‘Time and the
university’, *ACME : An International E-journal for Critical
Geographies* 10(3) (2011): 483–507.

Pasquale, Frank. *New Laws of Robotics*.

*Defending Human Expertise in the Age of AI*, Cambdrige. Harvard
University Press, 2020.

Phan, Thao, Goldenfein, Jake Mann, Monique, and Kuch, Declan. ‘Economies
of Virtue: The Circulation of “Ethics” in Big Tech’, *Science as
Culture* 31 (2022): 121–35.

Spade, Dean. ‘Solidarity Not Charity: Mutual Aid for Mobilization and
Survival’, *Social Text* 142(38) (2020): 131–51.

——. *Mutual Aid: Building Solidarity During This Crisis (and the Next)*,
London: Verso, 2020.

Wilson, Christo, Ghosh, Avjit, Jiang, Shan, Mislove, Alan, Baker, Lewis,
Szary, Janelle, Trindel, Kelly, and Polli, Frida. ‘Building and Auditing
Fair Algorithms: A Case Study in Candidate Screening’, Proceedings of
the FAccT Conference, March 1–10, 2021, Virtual Event, Canada, 1–10
March. 2021.

[^15Goldenfein_1]: Alex Hanna has since left Google and is presently Director of
    Research at the Distributed AI Research Institute (DAIR).

[^15Goldenfein_2]: See ‘Open Secrets’ in this collection.

[^15Goldenfein_3]: Timnit Gebru is a prominent computer scientist who was previously
    employed as the co-lead of the Google Research Ethical AI team. In
    December 2020, Google announced that she had resigned from her role,
    a claim that Gebru has denied, stating that she was fired following
    a dispute over an academic paper, in which she and other colleagues
    questioned the ethical risks of large language models.

[^15Goldenfein_4]: For more on the Koch brothers and political laundering see Tim
    Dickenson, ‘Inside the Koch Brothers Toxic Empire’, *Rolling Stone,*
    24 September 2014,https://www.rollingstone.com/politics/politics-news/inside-the-koch-brothers-toxic-empire-164403/.

[^15Goldenfein_5]: ‘Holding to Account: Safiya Umoja Noble and Meredith Whittaker on
    Duties of Care and Resistance to Big Tech’, *Logic: Beacons* 15
    (2022),https://logicmag.io/beacons/holding-to-account-safiya-umoja-noble-and-meredith-whittaker/.

[^15Goldenfein_6]: Thao Phan et al., ‘Economies of Virtue: The Circulation of
    “Ethics” in Big Tech’, *Science as Culture* 31 (2022): 121.

[^15Goldenfein_7]: FAccT is the acronym for the Association for Computing Machinery
    Fairness Accountability and Transparency network, who host an annual
    conference titled ACM FAccT.

[^15Goldenfein_8]: Referring to 1940 Statement of Principles on Academic Freedom and
    Tenure,
    https://www.aaup.org/report/1940-statement-principles-academic-freedom-and-tenure;
    Eli Meyerhff et al., ‘Time and the University’, *ACME: An
    International E-journal for Critical Geographies* 10(3) (2011): 483.

[^15Goldenfein_9]: J. Khadijah Abdurahman, ‘Encoding Hindutva: Shalini Kantayya and
    Coded Bias’ *Medium blog*, 2 June 2021,
    https://upfromthecracks.medium.com/encoding-hindutva-shalini-kantayya-and-coded-bias-721fe04f225f.

[^15Goldenfein_10]: J. Khadijah Abdurahman, ‘A Body of Work That Cannot be Ignored’
    *Logic: Beacons* 15 (2022)https://logicmag.io/beacons/a-body-of-work-that-cannot-be-ignored/.

[^15Goldenfein_11]: Frank Pasquale, *New Laws of Robotics: Defending Human Expertise
    in the Age of AI*, Cambridge: Harvard University Press, 2020.

[^15Goldenfein_12]: Dean Spade, ‘Solidarity Not Charity: Mutual Aid for Mobilization
    and Survival’, *Social Text* 142(38), (2020): 131.

[^15Goldenfein_13]: Dean Spade, *Mutual Aid: Building Solidarity During This Crisis
    (and the Next)*, London: Verso, 2020.

[^15Goldenfein_14]: Stefano Harney and Fred Moten, *The Undercommons: Fugitive
    Planning & Black Study*, London: Minor Compositions, 2013.

[^15Goldenfein_15]: J. Khadijah Abdurahman, ‘On the Moral Collapse of AI Ethics’
    *Medium blog* ,8 December 2020,
    https://upfromthecracks.medium.com/on-the-moral-collapse-of-ai-ethics-791cbc7df872.

[^15Goldenfein_16]: Sara Ahmed, *Complaint!*, Durham: Duke University Press, 2021.

[^15Goldenfein_17]: Christo Wilson et al., ‘Building and Auditing Fair Algorithms: A
    Case Study in Candidate Screening’, Proceedings of the FAccT
    Conference, Virtual Event, Canada, 1–10 March 2021.
<br/> 
