---
Pr-id: MoneyLab
P-id: INC Reader
A-id: 10
Type: article
Book-type: anthology
Anthology item: article
Item-id: unique no.
Article-title: title of the article
Article-status: accepted
Author: name(s) of author(s)
Author-email:   corresponding address
Author-bio:  about the author
Abstract:   short description of the article (100 words)
Keywords:   50 keywords for search and indexing
Rights: CC BY-NC 4.0
...


# Military Virtues and the Limits of ‘Ethics’ in AI Research

### Michael Richardson

War trades on virtue. The virtue of warriors, of just causes, of doing
what must be done in the face of adversity to sustain nation or
religion. War’s virtue extends to antiquity, at least in the West, but
its modern articulation bears distinct characteristics. Virtuous war is
now technological war, war applied with precision, information,
rationality, and proportionality bequeathed by technological revolutions
of logistics, science, and computation. In the shift currently underway
to autonomous weapons systems (AWS) and the incorporation of artificial
intelligence (AI) more generally into warfare, virtue functions to sell
publics and institutions on the necessity of ever more complex, more
codified, and more inscrutable emergent technologies. Virtue does not
simply flow from AWS and military AI, but is imbued in them by the
incorporation of laws and ethics within the systems themselves. Or so
the story goes. Just as the virtues of virtuality contributed to the
obscuring of the violence of America’s forever wars in the aftermath of
9/11, so too do the embrace and promotion of the virtues of autonomous
systems risk occluding the reproduction and intensification of existing
injuries and injustices, as well as the creation of new forms of
violence and oppression.

While Big Tech’s forays into U.S. military contracts tend to attract
controversy—think Google and the outrage over its work to apply
TensorFlow algorithms to drone image processing as part of the
Department of Defense’s (DoD) Project Maven initiative—interdependencies
between militaries and private and civil institutions are equally
pernicious in the thriving ecologies of start-ups, research translation
hubs, defence funding programs, government initiatives, cash-strapped
universities, and grant-hungry academics that can be found across the
globe. In these economies of virtue, ‘ethics’ serve not only to
facilitate mutually beneficial collaborations by cloaking military
violence but also as potential commodities, able to be coded into the
very technologies at hand.[^12Richardson_1] Defence researchers and companies can not
only *be* virtuous, but also can *make* war virtuous too. In this
chapter, I examine the emerging military technology industry in
Australia and its relation to academia to argue that military economies
of virtue operate in ways that are similar to and different from those
at work at the wider nexus of the tech sector and AI research in the
academy.

Militaries are well behind the private sector in AI and big data
development and expertise. This reality is accelerating collaborative,
industry-led processes that mimic aspects of the Silicon Valley model of
agile development, as the head of Pentagon’s Algorithmic Warfare
Cross-Functional Warfare Team (AWCFT) admitted on its formation in
2017.[^12Richardson_2] This move-fast-and-break-things approach could have serious
repercussions given the life and death situations in which military
technology is often applied. But militaries are not only valuable
clients for Big Tech, but also increasingly important sources of funding
for academic research. In the context of military AI, ‘ethics’ possesses
an economic function that frames, facilitates, and feeds engagements
between industry, academia, and military institutions. Military
technologies and especially weapons systems are often framed by distinct
ethics discourses, which emerge from a melange of the laws of armed
conflict, international humanitarian law, specific and predefined rules
of engagement, and—more nebulously—a warrior ethos. Militaries,
including the Australian Defence Force (ADF), tend to see ethics as
instrumental and principally related to conduct on and off the
battlefield by individual soldiers, rather than enmeshed with larger
questions of justice or societal obligation. Ethics are typically posed
as both values to hold and problems to solve. When this approach
encounters military AI, the limits of ‘ethics’ as a framework for
reducing harm become clear: ethics are already subordinated to martial
violence in that they are always concerned with enabling its infliction.

## Military Virtues

According to international relations scholar James Der Derian, the
growing centrality of computation to warfare that began in the Cold War
and accelerated dramatically in its aftermath signals the emergence of a
new mode of armed conflict led by the United States. ‘At the heart of
virtuous war,’ writes Der Derian, ‘is the technical capability and
ethical imperative to threaten and, if necessary, actualise violence
from a distance—*with no or minimal casualties*.’[^12Richardson_3] Of course,
virtuous war has not eliminated killing, nor the killing of civilians,
as the use of drones and autonomous systems by the United States and
Israel readily attests. Nor does virtuous war stop at violence itself:
war in Der Derian’s conception is also virtual, in the sense that it
depends more and more upon information and abstraction. While the
attempt to capture both ‘virtue’ and ‘virtual’ in his coining of a new
form of warfare is somewhat murky, his analysis nonetheless points to
the close relation between emergent forms of warfare dependent on
simulation, modelling, computation, automation, and autonomy and the
discursive refiguring of warfare that legitimises its centrality. As Der
Derian points out, virtuous war is produced by and in turn sustains an
amorphous array of agencies, actors, and institutions that he calls the
military-industrial-media-entertainment network, or MIME-NET.[^12Richardson_4]
Hovering close at hand is the university and its researchers.

Tight ties between militaries and academia are far from new, both in the
U.S. and elsewhere. During the Cold War, the Department of Defense
(DoD), Central Intelligence Agency (CIA), and the Defence Advanced
Research Projects Agency (DARPA) began directly funding research in the
U.S. and around the world across a host of disciplines, from nuclear
physics to psychology to medicine to anthropology. DARPA also funded the
Strategic Computing Initiative, which pumped over USD\$1bn into advanced
computation and artificial intelligence from 1983 to 1993. But military
funding was also critical to the very emergence of computation and
cybernetics, which laid the conceptual and mathematical foundation for
contemporary techniques of machine learning that are often packaged and
promoted as ‘Artificial Intelligence,’[^12Richardson_5] even if they largely depend
on human labour.[^12Richardson_6] Whether applied to military or civilian contexts,
contemporary techniques of machine learning depend on compute power that
often far exceeds the capacity of university labs, even when resources
are pooled between institutions. This has contributed to what Meredith
Whittaker calls the ‘capture’ of AI research by Big Tech, in which
researchers become dependent on access to platforms run by Amazon,
Google, and Microsoft, and so tend to undertake research that fits
within AI paradigms that reproduce the existing infrastructures as
beneficial and necessary.[^12Richardson_7] This dependence operates in the shadow of
recurring controversies surrounding the social, cultural, and political
impacts of Big Tech (and particularly Facebook, Google, and Amazon).

These controversies in turn intensify the need for ‘economies of virtue’
in which ‘virtue and ethics are the primary objects that are produced
and circulated by groups inside Big Tech—through the establishment of,
for example, ethics boards and working groups—and also outside, from
universities, research institutes, consultancies, and other allied
industries.’[^12Richardson_8] As the editors of this volume point out in their recent
provocation on the subject, this economy arises in a context of a
growing crisis in public funding of Western universities and of research
in particular, which has intensified dependencies such as that of AI
research on Big Tech dollars. Examination of these relations in a
commercial setting is critical, but military, national security, and
intelligence entanglements cannot be excised from the equation or
treated as a minor case study. While the critical scholarship on the
incorporation of AI into war and national security continues to grow,
the institutional role of universities and funding mechanisms in that
process demands more attention.[^12Richardson_9]

Universities, as Alison Howell persuasively argues, have not been
recently ‘militarised’ but have always been institutions produced by
martial politics and in service of martial ends.[^12Richardson_10] As noted above,
the internet more generally and AI itself owed and continue to owe much
to military funding and objectives, both in direct and indirect ways.
Today in the United States, one of the leading proponents of military AI
development and a cheerleader for an international AI arms race is
former Google CEO Eric Schmidt, now the head of Defence Technology
Innovation Board. For Schmidt and his fellow travellers, military AI
applications are always already virtuous precisely because they secure
the state against threat and strengthen its standing in the global
arena. All this could be understood as an attempt to securitize AI
itself, such that much of the work that happens in this sphere enters
into a domain outside ordinary politics and, in doing so, operates in
the exceptional space of security.[^12Richardson_11] The task, then, is to examine
*how* militaries participate in economies of virtue in AI research,
because the way they conceive and commodify ethics is often different
from civilian actors.

This chapter takes up this question of how militaries participate in
economies of virtue by examining the Australian context, rather than the
American. This examination is instructive because Australia is in the
early days of a deliberate strategic effort to accelerate its military
industries, both to provide homegrown technology and to produce a new
export sector. In military spending terms, Australia is a moderate
player, committing \$44.619 bn in 2021–22, or 2.09 percent of GDP and
1.4 percent of total global defence spending (compared to 39 percent by
the US and 13 percent by China), but with those figures set to grow
under commitments made by the Morrison government, including at least
\$70bn for nuclear powered submarines under the new tripartite AUKUS
arrangement.[^12Richardson_12] In export terms, Australia’s defence industry is
barely a player at all, with just \$5.5bn in exports in 2019–20. But
that figure is up from \$1.5bn in 2017–18, the direct result of a range
of government initiatives, targeted investment strategies, academic
research funding programs, and knowledge transfer hubs. Together, these
aim to hothouse military technology start-ups by echoing the
public–private partnerships so beloved by neoliberal infrastructure
builders. 
To show how military ethics functions within economies of
virtue, this essay argues that three critical dynamics around ‘ethics’
are shaping the emerging military technology industry in Australia and
its relation to academia. I begin by examining how ethics function as a
martial commodity in military technology start-ups, using the case study
of Cyborg Dynamics Engineering and its Athena AI platform. Next, I turn
to the discursive function that ethics serves in the growing defence
industries in Australia by examining the role of new Defence
Collaborative Research Centre (DCRC) initiatives, with a focus on the
Trusted Autonomous Systems DCRC in Brisbane, Queensland. Third, I make
the deliberately provocative proposal that ‘ethics’ facilitates
engagement with universities, with research funding as a central factor,
materialised through centres, networks, symposiums, and workshops. In
this context, ethics serves as a keyword of the
defence-academia-industrial complex. In closing, I argue that martial
conceptions of ethics and virtue are a distinct yet critical component
of economies of virtue that require further research and sustained
critical attention.

## Military Ethics as Code and Commodity

Ethics, laws, and codes abound in military context, from the Laws of
Armed Conflict (LOAC) to Codes of Conduct to more amorphous yet morally
forceful concepts such as the warrior ethos. States themselves are
constrained—in theory, if not always in practice—by international laws,
which determine both the instances in which war may be deemed just and
which protect the rights of civilians within conflict. When soldiers go
to war, ‘they are bound by a series of ethical principles that proscribe
particular actions and forbid others.’[^12Richardson_13] Virtuous conduct might not
be set out in such principles, but instead be culturally produced and
maintained within particularly armed services or units. We might think
of virtue as more closely tied with morality—with good character—whereas
ethics concerns behaviour and the limits of what is allowable in certain
circumstances. Both military ethics and virtue are not immutable but
rather have developed substantially over time, often in association with
the rise of new forms of warfare such as the proliferation of airpower
in the twentieth century. Conflated with codes and laws governing
conduct, ethics tend to be narrowly conceived in military contexts,
whereas soldiers can feel a more complex and intense relation to the
warrior ethos.

With the emergence of technologies for finding, selecting, targeting,
and killing, the potential to code ethics into computational systems has
proven alluring to military leaders. As Christian Enemark has observed,
the rise of drone warfare changes the ethical calculus of war for
states, as the lack of exposure of soldiers to risk and the arguable
reduction in civilian casualties promises to produce more ethical
warfare through technological advancement.[^12Richardson_14] Autonomous weapons
systems promise to further remove the fallible and flawed human
decision-making from the equation, transforming the codes of law,
ethics, and conduct into computational decision-trees that can be
applied in measured, flexible, and reliable fashion by autonomous
systems, whether ‘intelligent’ or not. Here, questions of ethics often
turn on the relation of a human operator to the kill
decision—in-the-loop, on the loop, or off-the-loop—or, more
fundamentally, on a moral insistence that killing in war should always
be a human decision.[^12Richardson_15] If contemporary developments in AWS and the
failure of the 2021 Convention on Conventional Weapons (CCW) to regulate
their use is an indication, the stance that views automated killing as
morally reprehensible is in grave trouble. There are huge philosophical
and practical implications of the increasing autonomy of warfare,
whether in terms of the material operation of AWS or in relation to the
algorithmic processes and thinking that underpin them, which threaten to
overwhelm the very possibility of law containing martial violence that
preempts and outpaces human capacities to think and decide.[^12Richardson_16]

But while scholars, activists, and publics around the world remain
apprehensive about the dangers of ‘killer robots,’ the question of
whether machines will decide on lethal actions has largely been decided
in practice. Here the distinct nature of military ethics actually
facilitates the emergence of ever-more autonomous technologies. Even if
they are embedded in a detailed social, institutional, historical, and
philosophical context (see, for example, the Australian Defence Force
2021 Military Ethics doctrine), military ethics still need to be
operationalized for the battlefield so that soldiers can make swift life
and death decisions. Understood as codes, ethics becomes codable—capable
of being translated into computational form, taught to intelligent
systems, and applied in specific contexts. As Elke Schwartz observes,
this ‘logic of an ethics module is reliant on a conception of ethics as
codifable, as ascertainable, and as producing clear, secure and,
ideally, certain outcomes.’[^12Richardson_17] Coded ethics becomes a commodity,
central to the sales pitch of start-ups and all too appealing to the
officers tasked with overseeing the development and procurement of
algorithmic and autonomous technologies.[^12Richardson_18]

The Australian start-up Cyborg Dynamics Engineering offers a telling
case study. Its flagship product is the Athena AI, a platform for
weapons targeting and battlefield analytics. As the company website
states, ‘Athena AI is one of the only vision-based AI systems on the
market that combines AI computer vision, AI enabled decision support and
display of the AI information in a user interface.’ While not itself an
autonomous weapons system, Athena AI is designed to augment human
targeting and, crucially, to provide object recognition and ethical and
legal evaluation tools. In public presentations and on the company
website, Cyborg Dynamics touts these ethical capabilities as critical
distinctions. In a short reflective academic article by the founder of
Cyborg Dynamics and collaborators at or affiliated with the Trusted
Autonomous Systems Defence Collaborative Research Centre describe the
technology as aiming to ‘augment human ethical and legal decision-making
on the battlefield by reducing the “fog of war,” and improving abidance
with international humanitarian law.’[^12Richardson_19] While that article
demonstrates the iterative and responsive approach to ethics undertaken
in the development of Athena AI, an extended quote from the company
website reveals how ethics becomes a value-add in marketing rhetoric:

> Athena AI is one of the only trusted AI products, having worked with
> International Weapons Review, military legal officers and military
> ethicists to help define a suitable data assurance and test
> methodology for AI vision and decision support certification. Our
> inbuilt decision support tools have legal and rules of engagement
> considerations where applicable.

Positioned first in a list of advantages for the system, this encoding
of ethics through the tying of LOAC, rules of engagement, and other such
codes to specific combat instances positions Athena AI as an improvement
upon the status quo. Familiar components of an economy of virtue are
evident too, through the participation of the boutique legal consultancy
International Weapons Review (IWR). As with the tech-critical entities
caught up in the economies of virtue described by this volume’s editors,
organisations like IWR are not ‘the problem’ per se—in IWR’s case, the
firm is run by experienced military lawyers with strong scholarly
standing—but are nonetheless part of the varied, evolving terrain of
military ethics economies.[^12Richardson_20] What goes unsaid in Cyborg Dynamics’
rhetoric is that the various detection and classification functions of
the platform—enabled, according to the company website, by multi-staged
neural networks—are implicitly legitimated and amplified by the legal
architecture that the system claims to provide. From a political
standpoint, ‘ethics’ here serves to lower the intensity of political
engagement by transforming concerns over military technologies into the
technocratic domain where computation meets law. But in industry, ethics
functions to boost the commodity value of the system: the system itself
is virtuous because it is already encoded with ethics and thus it
promises to make the armed services that deploy it more rigorous in
their adherence to ethical codes.

## Ethics and the Infrastructures of Research Translation

Companies like Cyborg Dynamics are part of a burgeoning ecology of small
and medium enterprises (SMEs) within the Australian defence industries.
Under the conservative leadership of former prime minister Malcolm
Turnbull, defence industry growth was deemed a crucial national
priority, both to reduce dependencies on foreign imports and to generate
jobs. However, the Australian Defence Forces are not large enough
purchasers to sustain a viable domestic defence industry. As the
Australian Department of Defence’s Defence Export Strategy states,
‘\[n\]ew markets and opportunities to diversify are required to help
unlock the full potential of the Australian defence industry to grow,
innovate and support Defence’s future needs.’[^12Richardson_21] Via the 2020 Defence
Strategic Update, then prime minister Scott Morrison committed
AUD\$270bn over the next decade to defence spending, aimed at increasing
and updating Australia’s military capacity and with significant
opportunities for industry and workers, with various skill training
programs designed to support naval shipbuilding and other defence
priorities.

Within this push, AI and other high-tech systems play an important role
in positioning the Australian defence industry as innovative,
forward-looking, and poised to contribute to the priorities of its
allies. The flagship project in this regard is the Boeing Airpower
Teaming System (ATS), described by the American defence giant on its
website as a ‘smart, uncrewed force multiplier.’ Developed in
collaboration with a number of Australian SMEs, the project—nicknamed
‘Loyal Wingman’—aims to develop a fast, attacking drone aircraft capable
of operating in support of human pilots engaged in dangerous missions,
allowing pilots to remain at a safe distance from high intensity
conflict zones or providing additional firepower in the event of an
aerial dogfight. As such, the ATS must operate with significant autonomy
for navigation, guidance, and targeting, which in turn demands
considerable expertise and opens up major ethical questions about the
use of force.

Situations like this are where research translation institutions such as
the Trusted Autonomous Systems Defence Cooperative Research Centre (TAS)
play a critical role, both in facilitating the involvement of Australian
enterprises and in foregrounding ethics in the design and promotion of
autonomous systems in defence. In operation since 1990, Cooperative
Research Centres (CRCs) are an Australian government initiative designed
to connect academic research with ‘industry-led’ projects, with funding
typically awarded in the tens of millions and over several years to a
partnership involving at least one industry and one university partner.
A number of CRCs have some crossover with national security, such as the
Data to Decisions (D2D) and Cyber Security CRCs, but are more oriented
to civilian concerns or, if securitized, more likely to be concerned
with defence-adjacent activities like law enforcement and signals
intelligence. The Defence Cooperative Research Centre is a more recent
subset, funded by the Next Generation Technologies Fund which has been
allocated \$730 million from 2016–17 to 2025–26 to invest ‘in
forward-looking game-changing capabilities aligned with Defence
priorities,’ according to a Department of Industry, Innovation and
Science fact sheet. As the first Defence Cooperative Research Centre,
TAS has received considerable resourcing and funding from the Australian
government as well as the Queensland state government, which is also the
only state to have a drone industry strategy. While the Boeing ATS
project has garnered by far the most media attention, the bread and
butter of TAS is smaller projects with SMEs, many based in Queensland.
But the CRC is also engaged in its own initiatives to develop assurance
and ethical frameworks for autonomous systems in defence.

A core component of this is their ‘Ethics and Law of Trusted Autonomous
Systems’ program, conducted in conjunction with University of
Queensland’s Future of War and Law Research Group. As the TAS website
states, their ‘Ethics Uplift Program engages diverse stakeholders to
provide evidence-based and practical risk management for ethical and
deployable AI in Defence.’ As in the economies of virtue that surround
Big Tech, the purpose of this program is not to question or critique the
foundational grounds of defence industries but to ‘produce ethics,
legal, safety and accountability frameworks for use of the
electromagnetic spectrum, robotics, autonomous systems and artificial
intelligence deployed within human-machine (HUM-T).’ Led by TAS Chief
Scientist Dr Kate Devitt, an ethicist by training with a track record of
robust critical scholarship on data and ethics, it can certainly be
argued that such initiatives should be understood in favourable terms as
doing crucial work to ensure that ethics are built into defence industry
projects and products from the beginning. An absence of such frameworks
would not stall initiatives, such an interpretation would argue, but
only mean that they go ahead with ethics less central to their
conception. There is merit in such claims, and TAS has also been able to
leverage its close relationship with the ADF to co-author ‘A Method for
Ethical AI in Defence,’ a technical report of the Defence Science &
Technology Group (DSTG), a military entity that funds, facilitates, and
prototypes new technology initiatives.

In the context of TAS, we need to understand ‘ethics’ as operating in at
least two modalities. The first is that outlined above, in which TAS
plays an infrastructural role in ensuring that ethical considerations
are foundational to military technologies developed in Australia. The
second modality sees these ethics initiatives as functioning
discursively to legitimate defence industries within academia and with
wider publics. In this sense, the Trusted Autonomous Systems Defence
Cooperative Research Centre can be understood as a kind of ethics
clearing house, connecting legal, philosophical and other humanities
research with military institutions, practitioners, and industry, with a
particular emphasis on start-ups. Doing so enables ‘ethics’ to be ‘built
in’ to AI and autonomous systems, with difficult questions around the
ethics of such technologies in the martial context pursued in
conjunction with or adjacent to their development. As the extensive list
of TAS-supported academic publications attests, TAS and its affiliated
researchers take theoretical and practical questions of ethics
seriously.[^12Richardson_22] Nor is TAS funded to generate large scale critique of
military operations or military spending as such, but rather to develop
home-grown defence industries that conform to Defence values and ethics.
As is often the case in innovation contexts, the presence of social
scientists and ethicists constitutes a kind of care work, which here
becomes care for the virtues of the nation and its Defence
endeavours.[^12Richardson_23] As such, TAS can be understood as a vital cog in
Australia’s economies of military virtue. This is not to dismiss out of
hand recent initiatives to deploy AI to identify cultural assets for
Western Yalanji peoples, help preserve Cape York languages, or develop
an autonomous marine vessel code of practice, but rather to recognise
that all such endeavours are bound up with the production and
commodification of virtue.

## Virtue, Academia, and ‘Ethics’ in Research Funding

Within the Australian academy, research funding has been placed under
increasing pressure over the last two decades, and particularly under
the conservative government in power since 2013. In 2014, competitive
grant funding by the Australian Research Council (ARC) stood at \$886m;
eight years later in 2022, it was \$815m. As a researcher fortunate
enough to receive ARC funding, I can attest to the luck involved in
having such grants awarded—especially in the humanities. A growing
government emphasis on impact, engagement with industry, and especially
research commercialisation has pushed funding more towards applied and
away from basic research. In 2022, the Morrison Government announced a
\$2.2bn fund dubbed ‘Australia’s Economic Accelerator, of which \$1.6bn
was earmarked for research that can be ‘commercialised’ in alignment
with National Manufactory Priorities, which include ‘defence and space’.
Within this context, ‘ethics’ functions as a keyword for the role of
humanities and social science (HASS) research, particularly in the
military sphere, as it enables claims of value within research spaces
otherwise focused on technological development. With defence and
national security framed as virtuous endeavours, ethics also provides a
common language with computer science, engineering, psychology, and
other disciplines more closely aligned historically to defence research.
Humanities research into communication, for example, can help do
information warfare the right way or assist in the development of
‘ethical’ autonomous battlefield systems.

Measuring the full extent of the impact of military funding on
Australian academic research is exceedingly difficult, even in general
terms. At the most prosaic level, there is the problem of classifying
so-called ‘dual use’ research, such as when the US Department of Defence
funds medical research. But there are also other challenges. How do you
define and delimit defence vs national security vs intelligence funding?
Can initiatives funded by the office of the prime minister, by cabinet,
or even by premiers be identified when explicit budgets are not
available? What about top secret initiatives? Or disparities between
budgeted amounts and actual expenditure? While Australian government
spending does entail certain degrees of transparency and accountability,
defence funding can be much more difficult to trace due to the scale,
secrecy, complexity, and overall opacity of the national security
elements of the state. That said, in Australia much of the more
overt—and substantive—defence funding flows from the Defence Science &
Technology Group (DSTG), an entity within Defence that seeks to
coordinate research priorities and provide an interface for both
academia and industry. Collaborative initiatives such as the Operations
Research Network (ORNet) directly address defence operations (command
and control, force design, operational planning, etc), while the Science
Partnerships (DSP) program provides a common framework for working with
defence and counts every public university in the country as a member.
At the state level, organisations such as the Defence Innovation Network
(in NSW and the ACT) or the Defence Science Institute (VIC) work closely
with DTSG to link up university researchers with SMEs around priority
problems. Academia thus engages with defence via an evolving
institutional infrastructure, which works to couch defence priorities in
the language of science and provide fora through which Defence personnel
can engage directly with researchers across a range of fields.

Virtue and ethics easily become the discursive and affective enablers of
increasingly militarised academic research. To take one example, the
influential Australian Strategic Policy Institute (ASPI) has publicly
advocated for much closer ties between academia and defence with a
strong focus on the virtues of national security. While ASPI is
constituted as a nonpartisan think tank, its agenda is firmly in line
with an expansive national security state and, beyond Australia’s
borders, with allied nations through the Five Eyes intelligence
partnership with the U.S., U.K., Canada, and New Zealand. In a series of
blogs, opinion pieces, and reports, ASPI chief executive Peter Jennings
and former chief defence scientist Robert Clark have called for a ‘Five
Eyes friendly’ university sector and the creation of an Australian
DARPA, the Pentagon’s famous Defence Advanced Research Projects Agency,
responsible for innovations ranging from the early internet to
retrofitting Hellfire missiles to Predator drones after 9/11.[^12Richardson_24]

Universities are often eager hosts for new defence initiatives. My own
institution, which runs the Australian Defence Force Academy in
Canberra, has the UNSW Defence Research Institute, the webpage of which
consists of gritty war-tech images and very little actual information,
including none at all about who is involved with the institute. More
often, though, defence initiatives are highly touted and full of
information. Our Trusted Autonomy research group and Institute for Cyber
Security, for example, are widely touted and active entities within the
university, and UNSW is well represented in the Cyber Security CRC
launched in 2018 with \$50m in government funding. Some of the scholars
involved in such initiatives are colleagues that I know and respect; so
again, my point is not to cast stones. After all, even if my own
research is not defence-funded, my university receives significant
income from the ADF and, like almost all Australian universities, relies
on state funding for both teaching and research. What’s important is
that on-the-ground infrastructure such as this is critical to meet the
kinds of cross-disciplinary problems posed by contemporary defence
challenges, as articulated most clearly in the Australian context by the
DSTG (Defence Science, Technology and Research Group) STaR shots which
include topics such as Agile Command and Control, Disruptive Weapons
Effects, and Information Warfare. This last is one area in which HASS
researchers on automation and AI are particularly appealing, as attested
to by the invitations for involvement in bids that I’ve received from my
own faculty. Despite being a humanities researcher critical of military
technology and militarization more generally, I often find myself in
strange circumstances—workshops, symposiums, and discussion groups with
defence-funded researchers or even defence personnel. For me, these are
valuable opportunities to see inside the system, and understand its
motivations and logics. My engagements with the Trusted Autonomous
Systems CRC, for example, have been deeply informative, not least for
the degree of insight they offer into the nuanced and even critical work
going on adjacent to and inside militaries.

The risk, however, is that in such contexts, ‘ethics’ becomes something
that HASS researchers can contribute to grant bids, while virtue
operates in the framing of such research as a national necessity that
can save lives and secure prosperous and safe futures. The slippery
nature of ‘ethics’ within these economies of virtues means that it can
simultaneously signify both the codable rules developed by computer
scientists and the processes, procedures, and fora produced by legal
scholars, philosophers, and communications researchers, to name a few.
Researchers who might otherwise be squeamish about doing ‘defence work’
can thus allow ‘ethics’ to insulate them from the kinetic operations and
lethal violence that are the animating ethos of militaries around the
world, Australia included. This in turn serves the interests of industry
and defence, as it produces buffers of virtue that cloud the brutality
at hand. ‘Ethics’ can thus be understood as a kind of floating
signifier, a malleable referent that attaches itself to a host of
situations and can readily be marshalled for martial ends.

## Conclusion

This nexus between the academy, defence, and industry should come as no
surprise: universities have always been martial institutions, bent to
martial ends and imbued with a martial politics. Universities are, after
all, institutions of empire and colony even more than they are sites of
learning, knowledge-making, and dissent. Yet the forms that this martial
nature takes change with the times, with technology, and with
ideological and economic sensibilities. In this chapter, I have argued
that in the convergence of the military, the university, and industry on
AI and autonomous technologies, a distinct form of economy can be
detected, in which ‘ethics’ functions as commodity and currency
dependent on context and ‘virtue’ draws heavily on military and statist
values. In the sketch I have attempted here of an evolving Australian
industry centred on new military technologies, ‘ethics’ greases the
wheels of collaboration, cloaks the violent purposes of defence, and yet
is always reducible in practice to a narrow and codable set of
prescriptions, drawn from a predefined body of laws and conventions
regarding armed conflict, weapons, and human rights. A shallow ‘ethics’
is nothing new, of course, but the crucial role it plays in the
Australian context matters. Mapping and analysing this confluence of AI
research, industry and application is a critical task because it
operates according to a different logic and economy of funding than is
the norm within tech support for academic research on AI and big data.
This military formation of ‘ethics’ has the potential to metastasize
into other contexts, particularly as cash-starved universities look to
one of the only remaining well-funded institutions in Australian public
life.

To ask the famous question: what is to be done? The growing enmeshment
of Big Tech in the American military establishment is hardly surprising,
given the history of technology translation between Silicon Valley and
DoD, but it has not been smooth. The 2018 Google Walkouts, sparked in
part by the company’s involvement in Project Maven, indicates one
potential fault line. High-skilled tech workers are more mobile than
most, with high demand for their skills and so possess more leverage
than individuals in most industries. But despite the high profile of the
Walkouts and Google’s very public backdown on Project Maven, the
fundamental relationship between tech and militarism has not changed
substantially. When TAS was announced as an initiative at QUT, students
launched a \#booksnotbombs campaign (Figure 1) that focused on the
inclusion of military giants BAE and Thales within the CRC funding
model. While the campaign didn’t succeed, it and the Walkouts do suggest
the necessity of collective responses within academia and tech to the
growing influence of military dollars in both domains. For critical
academics working in this space, one vital step is to follow the
money—not by accessing military funding, but by mapping its movement
through the university and para-academic system. Conducting such a
forensic exercise would no doubt demand collective labour, as well as
the formation of new networks of knowledge. Undertaking that project
would not, of course, undo or even slow the operation of this particular
economy of virtue. But it would expose the scale of the problem and move
from the mix of general claims and specific instances articulated here
into a more robust critique of how virtue operates at the nexus of
militaries, academic, and AI research.


![Fig 1: Banner from the Disarm QUT campaign. Image credit: Monique
Mann.](imgs/12.1.jpg)

<br/>


## Acknowledgments

In addition to the editors and reviewers who provided astute guidance on
this piece, I am deeply grateful to Dr Kate Devitt, who offered
detailed, unflinching, and constructive feedback on an earlier version.
Kate’s willingness to engage with an essay critical of TAS is testament
to her own ethical rigour and generosity of thought but should not be
read as an endorsement of its arguments. All errors that remain are my
own.

## Funding Disclosure 

This research was funded by ARC DECRA DE190100486, an Australian
government initiative. It was also supported by the University of New
South Wales, which is a member of the Defence Science Program and
institutional home of the Australian Defence Force Academy.

## References

Amoore, Louise. ‘Algorithmic War: Everyday Geographies of the War on
Terror’, *Antipode* 41.1 (2009): 49–69.

Amoore, Louise. *Cloud Ethics: Algorithms and the Attributes of
Ourselves and Others*. Durham: Duke University Press, 2020.

Australian Defence Force. *ADF Philosophical Doctrine—Military Ethics.
Australian Defence Force*, 2021,
https://theforge.defence.gov.au/adf-philosophical-doctrine-military-ethics.

Australian Department of Defence, *Defence Export Strategy*,
Commonwealth of Australia, 2018.

Beard, Matthew. ‘Beyond Tallinn: The Code of the Cyberwarrior?’ in
Allhoff F, Henschke A, and Strawser BJ (eds) *Binary Bullets: The Ethics
of Cyberwarfare*, New York: Oxford University Press, 2016, pp. 139–56.

Bellanova Rocco, Jacobsen, Katja Lindskov, and Monsees, Linda. ‘Taking
the trouble: science, technology and security studies’, *Critical
Studies on Security* 8.2 (2020): 87–100.

Boeing. ‘Airpower Teaming System’,
https://www.boeing.com/defense/airpower-teaming-system/index.page.

Buzan, Barry, Wæver, Ole and de Wilde Jaap. *Security: A New Framework
for Analysis*. Boulder: Lynne Reiner Publishers, 1998.

Clark, Robert and Jennings, Peter. ‘An Australian DARPA to Turbocharge
Universities’ National Security Research: Securely Managed
Defence-funded Research Partnerships in Five-Eyes Universities’, ASPI,
2021.

Department of Defence. ‘Memorandum for the Establishment of an
Algorithmic Warfare Cross-Functional Team (Project Maven)’, 2017,
https://www.govexec.com/media/gbc/docs/pdfs\_edit/establishment\_of\_the\_awcft\_project\_maven.pdf.

Department of Industry. ‘Growth opportunities. Department of Industry,
Science, Energy and Resources’, 2021,
https://www.industry.gov.au/data-and-publications/defence-national-manufacturing-priority-road-map/growth-opportunities.

Der Derian, James. *Virtuous War: Mapping the
Military-Industrial-Media-Entertainment Network*, 2nd edition, New York:
Routledge, 2009.

[[]{#OLE_LINK30 .anchor}]{#OLE_LINK29 .anchor}Devitt, S. Kate.
‘[[]{#OLE_LINK43 .anchor}]{#OLE_LINK42 .anchor}Normative Epistemology
for Lethal Autonomous Weapons Systems’ in [[]{#OLE_LINK34
.anchor}]{#OLE_LINK33 .anchor}Galliott, Jai, MacIntosh, Duncan and
Ohlin, Jens David (eds) *Lethal Autonomous Weapons: Re-Examining the Law
and Ethics of Robotic Warfare*, Oxford: Oxford University Press, 2021,
pp. 237–58.

Devitt, S. Kate, Scholz, Jason, Schless, Timo and Lewis, Larry
(preprint). [[]{#OLE_LINK32 .anchor}]{#OLE_LINK31 .anchor}‘Developing a
Trusted Human-AI Network for Humanitarian Benefit’, *Journal of Digital
War* ‘My War’ special issue, 2022.

Enemark, Christian. *Armed Drones and the Ethics of War: Military Virtue
in a Post-Heroic Age*, New York: Routledge, 2014.

Galliott, Jai, MacIntosh, Duncan and Ohlin, Jens David (eds). *Lethal
Autonomous Weapons: Re-Examining the Law and Ethics of Robotic Warfare*,
Oxford, New York: Oxford University Press, 2021.

Halpern, Orit. *Beautiful Data: A History of Vision and Reason since
1945*, Durham: Duke University Press, 2015.

Howell, Alison. ‘Forget “Militarization”: Race, Disability and the
“Martial Politics” of the Police and of the University’, *International
Feminist Journal of Politics* 20.2 (2018): 117–36.

Liljefors, Max, Gregor Noll, and Daniel Steuer (eds). *War and
Algorithm*, London; New York: Rowman & Littlefield International, 2019.

Massingham, Eve. ‘Automation of the Spectrum, Automation and the
Spectrum: Legal Challenges When Optimising Spectrum Use for Military
Operations’, *Law, Technology and Humans* 3.2 (2021): 91–106.

Ministers for the Department of Industry, Science, Energy and Resources.
‘Action Plan to supercharge research commercialisation’, 2022,
https://www.minister.industry.gov.au/ministers/taylor/media-releases/action-plan-supercharge-research-commercialisation.

Packer, Jeremy and Joshua Reeves. *Killer Apps: War, Media, Machine*,
Durham: Duke University Press, 2020.

Phan, Thao, Jake Goldenfein, Monique Mann, and Declan Kuch. ‘Economies
of Virtue: The Circulation of ‘Ethics’ in Big Tech’, *Science as
Culture* 31.1 (2021): 1–15.

Roberson, Tara, Stephen Bornstein, Rain Liivoja, Simon Ng, Jason Scholz,
and Kate Devitt. ‘A Method for Ethical AI in Defence: A Case Study on
Developing Trustworthy Autonomous Systems’, *Journal of Responsible
Technology* 11 (2022).

Sadowski, Jathan. ‘Potemkin AI.’ *Real Life*, 6 August 2018,
https://reallifemag.com/potemkin-ai/.

Schwarz, Elke. *Death Machines: The Ethics of Violent Technologies*,
Manchester: Manchester University Press, 2018.

———. ‘Silicon Valley Goes to War: Artificial Intelligence, Weapons
Systems, and Moral Agency’, *Philosophy Today* 65.3, (2021): 549–69.

Sharkey, Noel. ‘Automating Warfare: Lessons Learned from the Drones.’
*Journal of Law, Information and Science* 21.2 (2011): 140–54.

Shepherd, Tory. ‘Australia’s Aukus Nuclear Submarines Could Cost as Much
as \$171bn, Report Finds’ *Guardian*, 13 December 2021, sec. World news,
https://www.theguardian.com/world/2021/dec/14/australias-aukus-nuclear-submarines-estimated-to-cost-at-least-70bn.

Suchman, Lucy. ‘Algorithmic Warfare and the Reinvention of Accuracy’,
*Critical Studies on Security* 8.2 (2020): 1–13.

Suchman, Lucy, Karolina Follis, and Jutta Weber. ‘Tracking and
Targeting: Sociotechnologies of (In)Security’, *Science, Technology, &
Human Values* 42.6 (2017): 983–1002.

Viseu, Ana. ‘Caring for Nanotechnology? Being an Integrated Social
Scientist’, *Social Studies of Science* 45.5 (2015): 642–64.

Whittaker, Meredith. ‘The Steep Cost of Capture’, *Interactions* 28.6
(2021): 50–5.

[^12Richardson_1]: Thao Phan et al., ‘‘Economies of Virtue: The Circulation of
    ‘Ethics’ in Big Tech,’’ *Science as Culture* 31.1 (2021): 121–35.

[^12Richardson_2]: Department of Defence, ‘Memorandum for the Establishment of an
    Algorithmic Warfare Cross-Functional Team (Project Maven),’ 26 April
    2017,
    https://www.govexec.com/media/gbc/docs/pdfs\_edit/establishment\_of\_the\_awcft\_project\_maven.pdf.

[^12Richardson_3]: [[]{#OLE_LINK36 .anchor}]{#OLE_LINK35 .anchor}James Der Derian,
    *Virtuous War: Mapping the Military-Industrial-Media-Entertainment
    Network*, 2nd edn., New York: Routledge, 2009, p. xxxi.

[^12Richardson_4]: Der Derian, *Virtuous War*, p. 83.

[^12Richardson_5]: Orit Halpern, *Beautiful Data: A History of Vision and Reason
    since 1945*, Durham: Duke University Press, 2015.

[^12Richardson_6]: Jathan Sadowski, ‘Potemkin AI,’ *Real Life*, 6 August 2018,
    https://reallifemag.com/potemkin-ai/.

[^12Richardson_7]: Meredith Whittaker, ‘The Steep Cost of Capture,’ *Interactions*
    28.6 (2021): 50–55.

[^12Richardson_8]: [[]{#OLE_LINK38 .anchor}]{#OLE_LINK37 .anchor}Phan et al.,
    ‘Economies of Virtue,’ p. 2.

[^12Richardson_9]: See, for example, Louise Amoore, ‘Algorithmic War: Everyday
    Geographies of the War on Terror,’ *Antipode* 41, no. 1 (2009):
    49–69; Louise Amoore, *Cloud Ethics: Algorithms and the Attributes
    of Ourselves and Others* Durham: Duke University Press, 2020; Rocco
    Bellanova, Katja Lindskov Jacobsen, and Linda Monsees, ‘Taking the
    Trouble: Science, Technology and Security Studies’, *Critical
    Studies on Security* 8.2 (2020): 87–100; Jeremy Packer and Joshua
    Reeves, *Killer Apps: War, Media, Machine*, Durham: Duke University
    Press, 2020; Lucy Suchman, ‘Algorithmic Warfare and the Reinvention
    of Accuracy,’ *Critical Studies on Security* 8.2 (2020): 1–13; Lucy
    Suchman, Karolina Follis, and Jutta Weber, ‘Tracking and Targeting:
    Sociotechnologies of (In)Security’, *Science, Technology, & Human
    Values* 42.6 (2017): 983–1002.

[^12Richardson_10]: Alison Howell, ‘Forget “Militarization”: Race, Disability and the
    “Martial Politics” of the Police and of the University’,
    *International Feminist Journal of Politics* 20.2 (2018): 117–36.

[^12Richardson_11]: Barry Buzan, Ole Wæver, and Jaap de Wilde, *Security: A New
    Framework for Analysis*, Boulder: Lynne Reiner Publishers, 1998.

[^12Richardson_12]: Science Department of Industry, ‘Growth opportunities,’ Text,
    Department of Industry, Science, Energy and Resources, Department of
    Industry, Science, Energy and Resources, 30 March 2021,
    https://www.industry.gov.au/data-and-publications/defence-national-manufacturing-priority-road-map/growth-opportunities;
    Ministers for the Department of Industry, Science, Energy and
    Resources, ‘Action Plan to Supercharge Research Commercialisation,’
    2 February 2 2022,
    https://www.minister.industry.gov.au/ministers/taylor/media-releases/action-plan-supercharge-research-commercialisation;
    Tory Shepherd, ‘Australia’s Aukus Nuclear Submarines Could Cost as
    Much as \$171bn, Report Finds,’ *Guardian*, 13 December 2021, sec.
    World news,
    https://www.theguardian.com/world/2021/dec/14/australias-aukus-nuclear-submarines-estimated-to-cost-at-least-70bn.

[^12Richardson_13]: Matthew Beard, ‘Beyond Tallinn: The Code of the Cyberwarrior?’ in
    *Binary Bullets: The Ethics of Cyberwarfare*, Fritz Allhoff, Adam
    Henschke, and Bradley Jay Strawser (eds), New York: Oxford
    University Press, 2016, p. 139.

[^12Richardson_14]: Christian Enemark, *Armed Drones and the Ethics of War: Military
    Virtue in a Post-Heroic Age*, London: Routledge, 2014.

[^12Richardson_15]: Elke Schwarz, *Death Machines: The Ethics of Violent
    Technologies*, Manchester: Manchester University Press, 2018; Noel
    Sharkey, ‘Automating Warfare: Lessons Learned from the Drones’,
    *Journal of Law, Information and Science* 21.2 (2011): 140–54.

[^12Richardson_16]: See, for example, Max Liljefors, Gregor Noll, and Daniel Steuer
    (eds), *War and Algorithm*,(London; New York: Rowman & Littlefield
    International, 2019.

[^12Richardson_17]: Schwartz, *Death Machines*, p. 16.

[^12Richardson_18]: Elke Schwarz, ‘Silicon Valley Goes to War: Artificial
    Intelligence, Weapons Systems, and Moral Agency’, *Philosophy Today*
    65.3 (2021): 549–69.

[^12Richardson_19]: Tara Roberson et al., ‘A Method for Ethical AI in Defence: A Case
    Study on Developing Trustworthy Autonomous Systems’, *Journal of
    Responsible Technology* 11 (2022): 1.

[^12Richardson_20]: Phan et al., ‘Economies of Virtue’, p. 10.

[^12Richardson_21]: Australian Department of Defence, *Defence Export Strategy*,
    Commonwealth of Australia, 2018.

[^12Richardson_22]: See, for example, S. Kate Devitt, ‘Normative Epistemology for
    Lethal Autonomous Weapons Systems’ in Galliott, Jai, MacIntosh,
    Duncan and Ohlin, Jens David (eds) *Lethal Autonomous Weapons:
    Re-Examining the Law and Ethics of Robotic Warfare*, Oxford: Oxford
    University Press, 2021, pp. 237–58; S. Kate Devitt et al.,
    ‘Developing a Trusted Human-AI Network for Humanitarian Benefit,’
    (preprint); Jai Galliott, Duncan MacIntosh, and Jens David Ohlin
    (eds), *Lethal Autonomous Weapons: Re-Examining the Law and Ethics
    of Robotic Warfare*, Ethics, National Security, and the Rule of Law,
    Oxford; New York: Oxford University Press, 2021; Eve Massingham,
    ‘Automation of the Spectrum, Automation and the Spectrum: Legal
    Challenges When Optimising Spectrum Use for Military Operations’,
    *Law, Technology and Humans* 3. 2 (2021): 91–106; Tara Roberson et
    al., ‘A Method for Ethical AI in Defence: A Case Study on Developing
    Trustworthy Autonomous Systems’.

[^12Richardson_23]: Ana Viseu, ‘Caring for Nanotechnology? Being an Integrated Social
    Scientist,’ *Social Studies of Science* 45.5 (2015): 642–64.

[^12Richardson_24]: Robert Clark and Peter Jennings, ‘An Australian DARPA to
    Turbocharge Universities’ National Security Research: Securely
    Managed Defence-Funded Research Partnerships in Five-Eyes
    Universities,’ *Australian Strategic Policy Institute blog*, 14 July
    2021,
    https://www.aspi.org.au/report/australian-darpa-turbocharge-universities-national-security-research-securely-managed.
