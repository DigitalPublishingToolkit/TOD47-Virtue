---
Pr-id: MoneyLab
P-id: INC Reader
A-id: 10
Type: article
Book-type: anthology
Anthology item: article
Item-id: unique no.
Article-title: title of the article
Article-status: accepted
Author: name(s) of author(s)
Author-email:   corresponding address
Author-bio:  about the author
Abstract:   short description of the article (100 words)
Keywords:   50 keywords for search and indexing
Rights: CC BY-NC 4.0
...


# Everyday AI Ethics: From the Global to Local Through Facial Recognition

### Angela Daly 

## Introduction

Prominent discussions on AI ethics frameworks and other initiatives,
take place at the international or national level, and especially those
from the human rights approach may claim a universal or global
application and significance.[^10Daly_1] Outside of prominent countries such as
those in North America, Europe, and East Asia,[^10Daly_2] national—and within
even the ‘prominent countries’, subnational (e.g. devolved regional or
provincial administrations), and local level—discussions and activities
around AI ethics have received less attention and instead are often
overlooked in favor of supposedly more impactful, ‘higher-level’
discussions. However, this is a problem, as these higher-level
discussions do not make much sense unless we have an understanding of
how AI is encountered, negotiated, and contested on local levels.[^10Daly_3]

Even within such prominent countries and regions, more local AI ethics
discussions and practices may be overlooked or deemed less relevant and
impactful for researchers, and possibly inconvenient for policy makers
and corporations. Looking at the U.K. context where I am now based and
which this chapter relates to, ‘impact’ in academia means ‘the
demonstrable contribution that excellent research makes to society and
the economy.’[^10Daly_4] Research impact policy in the U.K. has led to research
critical of government policy receiving lower scores than other kinds of
policy-related research, and has been perceived by some academics ‘to
bias research funding towards the interests of political ideology and
big business.’[^10Daly_5] The apparent national and international importance of
certain AI ethics activities seem also to have attracted research and
other forms of funding, at least partly on this presumably ‘impactful’
basis and the ensuing ‘economy of virtue’ whereby AI ethics is funded by
Big Tech and produces output for Big Tech’s consumption.[^10Daly_6]

Indeed, while I hold research projects funded by UK Research and
Innovation (UKRI) on automation and AI topics, I am writing this paper
on an ‘unfunded’ basis as it does not fit with the scope of these other
projects. The UKRI is the U.K.’s public research funding body, but has a
strong emphasis on ‘commercialisation’ guided by policies which lead to,
as Finn puts it, ‘a commodification of domestic UK innovation.’[^10Daly_7]
Other work I’ve done on facial recognition and Scotland has also been
during my non externally–funded research time allocated by my university
employer and during my own time outside of official working hours. I
believe this says something about competitive funding priorities in
academic research that critical work on facial recognition in a more
localized context of Scotland is not as attractive as research aiming to
facilitate uses of AI and automation in health and manufacturing in the
U.K. (for which I have received funding). This insight adds to those
identified by other authors in this collection such as corporate
priorities, government priorities and gender (and likely other)
imbalances in who receives funding.[^10Daly_8] However, this contribution also
bears out Edwards’ view that unfunded research is ‘a space in which to
confront and address the tensions generated by forms of academic
identity pulling in different directions.’[^10Daly_9] In my case, this meant
giving me the opportunity to make ‘a creative and intellectually-driven
contribution to knowledge’[^10Daly_10] and resist my own neoliberal success in
AI grant generation!

This paper also looks critically at AI ethics in the U.K. As mentioned
above, critiques of U.K. government policy may score lower in research
impact compared to other policy-oriented research. The U.K. government
has invested heavily in AI, including in governance and policy aspects,
supporting directly or indirectly a constellation of actors and
initiatives such as the Alan Turing Institute, the Digital Catapult, and
the Centre for Data Ethics and Innovation. The Ada Lovelace Institute,
while ostensibly independent, was established ‘in collaboration’ with a
number of U.K. government–funded bodies, including the Alan Turing
Institute, and has received funding from UKRI. The U.K. has been active
as a nation-state in global AI governance discussions as well as
domestically with its own National AI Strategy, and more recently a
policy paper outlining its ‘pro-innovation approach to regulating AI,’
which eschews legally binding norms in the process.[^10Daly_11] The U.K.’s
current AI approach is underpinning by a number of themes including a
prioritising of ‘innovation’ and a cleavage with the European Union’s
approach to data protection, moving closer to that of the U.S., both
related to the U.K.’s post-Brexit geopolitical and economic stance.[^10Daly_12]
Ossewaarde and Gulenc find the British AI approach to be digitally
utopian, technologically solutionist and leveraging British imperialism
and leadership in the Industrial Revolution to project the U.K. as a
neo-imperial post-Brexit ‘world leader’ in AI in the future, while
glossing over the potentially de-democratizing ‘dark side’ of AI.[^10Daly_13]

Furthermore, Ossewaarde and Gulenc identify a strong technocratic
character to the U.K.’s AI policy.[^10Daly_14] The very people involved in AI
governance and ethics discussions and formulating any principles or
rules are often ‘technically oriented’ experts, far removed from
ordinary people and their experiences, therefore rendering AI governance
a hitherto ‘elitist project.’[^10Daly_15] AI ethics are also ‘primarily shaped
by men,’ exhibit a more general ‘lack of diversity,’[^10Daly_16] and are
usually ‘framed by means of Western values, contexts, and
concerns.’[^10Daly_17]

I want to turn attention away from this somewhat elitist affair of
devising high level (in various senses) AI principles to looking more at
localized, everyday encounters with AI technologies and AI ethics which
are manifesting in different parts of the world in response to actual
problems with AI. I do this through the lens of a particular application
of AI, in the form of facial recognition cameras and software,
especially when used by law enforcement. This is a concrete example of
localized engagements with AI and the formation of resistance which have
led to forms of localized governance of AI in some places including the
UK. Despite the lofty ideals and potential for large scale impact that
more global initiatives on AI ethics and governance promise, and despite
a more global approach probably being more appropriate for a globalized,
transnational technology such as AI and applications including facial
recognition, it is the everyday, localized encounter with AI
technologies and AI ethics I consider in this chapter. The local and
everyday have been largely overlooked and neglected by much of the AI
ethics literature and activity to date, possibly due to the less
‘impactful’ perception of such encounters. Yet without an understanding
of these local encounters, high-level AI ethics remain abstract, adrift,
and often apolitical.

In any event, these everyday encounters are impactful in other ways when
individuals and communities negotiate and contest certain AI uses in
ways that may lead to change as policymakers and the law may respond to
their wishes. This is clearly impactful in localities where it takes
place but lacks acknowledgement and claims, whether implicit or
explicit, to universality that conventional high-level AI ethics
initiatives contain, and which is incentivized by impact in academic
research.[^10Daly_18] In the case of facial recognition at least, and perhaps
more broadly, more AI ‘ethical’ attention given to this application in
its local and everyday encounters can highlight or serve forms of
activism, resistance, or critique, whereas ethical attention that aims
at the more abstracted, higher or ‘universal’ level is frequently more
in service of forces of capital and political power.[^10Daly_19]

I start by considering the ways in which AI is an everyday technology
already. I concentrate on facial recognition as an example of everyday
AI that has invoked contestations over its use, and in some places
resulted in curbs on it, with a particular focus on the U.K. Overall,
this shows that a key point of encounter with AI, and thus a key site of
ethical, legal, and political interrogation, is and must be the point at
which individuals and communities engage with, and in some cases such as
facial recognition, contest AI.[^10Daly_20] Moving beyond the technocratic high
level AI ethics norm formation, a consideration of these everyday
encounters, including protest, social movements, and legal mobilization
through litigation must be part of the AI ethics discussion, especially
when, as in the case of the U.K., the everyday paints a different
picture to the imaginaries of the U.K.’s high level AI strategies and
policies.

## AI as an Everyday Technology

AI is becoming an everyday technology throughout the world, although it
is often not considered in this way. The idea of the everyday in AI, and
people’s everyday practices and experiences of AI, has been considered
by some authors, including Burgess, Mitchell, and Highfield, who have
aimed to:

> get beyond the current hype and anxieties around self-driving cars,
> algorithms and robotics, and to achieve a more precise and grounded
> understanding of exactly what might be meant by automation, how and
> with what effects it is becoming entangled with everyday life and how
> investigating these relationships also helps us understanding
> processes of media change in society more broadly.[^10Daly_21]

Further, Pink et al. recognize:

> \[d\]iscussion of these automated technologies is often shrouded with
> narratives which highlight extreme and spectacular examples, rather
> than the ordinary mundane realities that characterise the overwhelming
> majority of people’s actual encounters with them.[^10Daly_22]

As AI is penetrating our everyday lives, albeit in different ways and
different contexts, this focus on the quotidian departs from much of the
literature and other discussions on AI,[^10Daly_23] which concentrates on the
more global or abstracted levels—and also often occurs at a more elite
level, as identified by Hagendorff above. It is the everyday where
encounters with AI occur, even if that everyday encounter may look
different in different scenarios.

However, it is also the everyday where people can fight back against
technologies, including AI and automation, despite the passivity often
implied by debate and literature. For Pink et al:

> The ordinary citizen is represented as passively in thrall to
> manipulation and exploitation of the proponents of the digital data
> economy. Yet, the automation logic is not the same everywhere—nor does
> it operate with the same kind of intensity on every occasion of use or
> every geographical location. People can and do resist\[…\][^10Daly_24]

As well as the encounter with AI for many if not most people being
primarily on this everyday, localized level, much of the AI governance
with ‘bite’ is also happening at this level, and, I argue, it has been
overlooked by much of the AI debates to date. This governance can be
shaped by individuals and communities encountering AI, negotiating it
and in some cases resisting it, as they do with other data-driven
surveillance technologies.[^10Daly_25] It is this which I turn to later, by
looking at how AI ethics is playing out at a grounded, local level, and
how this relates, or not, to the ‘higher-level’ discussions and
formulations of AI ethics, through the lens of facial recognition.
First, I consider what an everyday law and ethics of AI means by
engaging with ideas of the everyday from legal studies.

## Turning from AI Ethics to Law to the Everyday

Considerations of law- and norm-making need to be brought into this idea
of everyday AI, as in some cases everyday negotiations and contestations
of norms address AI ethics in more impactful or satisfactory ways than
the higher level, abstracted AI ethics activities we have seen in recent
years.

The turn to such high-level ethics initiatives in AI has been criticized
by Wagner as ‘ethics washing’ since the ethics statements and
initiatives usually lack legal or other forms of enforceability and
accountability in their implementation.[^10Daly_26] So, instead of being a
complement for binding rights and responsibilities, they are a
substitute for them. It is important to note that ethics is used in a
specific way in the context of AI governance—i.e., to promote lists of
non-binding norms often by nation-states and large corporations—and
critiques of ethics relate to that specific situation and use, but
ethics has a broader meaning since law and other normative schemes are
also manifestations of applied ethics.[^10Daly_27]

Yet legal enforceability of AI norms is not necessarily sufficient or
appropriate alone to address issues pertaining to the unenforceability
of AI ethics principles, since the content of those norms as well as
their enforceability needs to be ‘good.’[^10Daly_28] The Trump Administration
in the U.S. adopted legally binding Executive Orders on AI, which
mandated a deregulatory approach to the technology, an outcome with
which critics of non-binding AI ethics are unlikely to seek or be
satisfied.[^10Daly_29] In any event, there are few legally enforceable AI
ethics/governance initiatives, and those that do exist are not at the
international level, but regional or national level instead.

At the international level, UNESCO member states recently adopted its
Recommendation on the Ethics of Artificial Intelligence. This is
significant since it is the first global standard on the topic, however
it is not binding on signatory states, and it is merely ‘recommended’
that member states implement it on a ‘voluntary basis’ in their
respective domestic jurisdictions.[^10Daly_30] Much attention so far has been
paid to efforts in the European Union (EU) to formulate its own
legislation on AI, the EU AI Act, which is currently under discussion at
the time of writing,[^10Daly_31] and is notable as the first major attempt by a
leading global jurisdiction to regulate AI in a binding way, albeit one
as it currently stands that will not outlaw completely law enforcement
use of facial recognition.[^10Daly_32]

Here, though, I want to look at more everyday understandings,
negotiations, and resistance of AI ethics norms and law, at the local or
microcosmic rather than national or international level. In doing this,
I seek to connect with scholarship on ‘everyday law’ or ‘legal
socialisation’ in how people experience, form and respect (legal)
norms,[^10Daly_33] or as Sarat and Kearns put it, ‘how law’s consumers produce
their own law and, in so doing, transform and reproduce state law.’[^10Daly_34]
This is because these understandings, negotiations, and resistances to
AI uses—especially by the state and corporations—emanating from
individuals and communities give us a sense of what AI uses people
notice and what they find acceptable/unacceptable, which may in turn
influence state law and corporate practices. Facial recognition
technology is notable as its use has provoked physical protests in
various parts of the world, in different contexts, and its use has
formed the basis of litigation and policy change in the U.K.

On this point, I also want to link this discussion of everyday law to
how law interacts with social movements and protest, an area
understudied both by social movement scholars and legal scholars.[^10Daly_35]
This is significant for facial recognition as protest and campaigning
have built up pressure, resulting in prohibitions or moratoriums on the
practice, and contested its use through litigation. This also connects
with the work done on ‘data activism’ by Milan and others, ‘which
critically engages with the manifold impact of data on social life’ and
includes ‘for instance, socio-technical practices that provide
counter-hegemonic responses to the discrimination, social exclusion and
privacy infringement that go hand in hand with big data’.[^10Daly_36] Data
activism has a particular emphasis on the ‘grassroots contentious
processes \[vis-à-vis datafication\] expressed by laypersons,
nongovernmental organizations and social movement networks alike.’[^10Daly_37]
Opposition to facial recognition both in social movement responses and
legislation and policy responses constitute what Kazansky terms
‘resistance to data-driven surveillance.’[^10Daly_38] Yet protest, social
movements, and law/policy change have rarely been viewed in concert in
the literature in this area on new technologies, especially AI.

I introduce these concepts as a backdrop for my inquiry into facial
recognition as an everyday AI technology creeping into the lives of
people around the world, and as a site of social movement data activist
contestations that interact with the law and ethics of AI. More
theoretical and empirical work is warranted on AI, activism, and ethics
(including law) to give a deeper understanding, especially from the
quotidian perspective of how normal, everyday people encounter and
engage with these issues. Here I seek to introduce these topics, but
more work could be done directly e.g. with those who influence,
negotiate and in particular resist facial recognition from everyday
perspectives and who are not typically involved in the ‘higher level’ AI
ethics initiatives and norm forming.

Everyday AI law, ethics and protest is already a practical reality, as
we see through examples such as demonstrations in England against the
Department of Education about unfair outcomes in school leaving results
in 2020 when they were determined by an algorithm (as traditional exams
were cancelled due to the COVID-19 pandemic), at which young people
chanted and held up placards saying ‘Fuck the Algorithm’. Kaun considers
this as an example of Willim’s ‘mundanization’ of digital technologies
i.e., ‘developing everyday understandings of complex technologies that
have implications for our everyday lives’.[^10Daly_39] The use of algorithms in
the public sector has provoked broader controversies, such as the
RoboDebt welfare surveillance scandal in Australia.[^10Daly_40] Further
examples of everyday AI ethics be found during the 2020-2021 Indian
farmers’ protests where farmers understood the connections between plans
for conglomerate Jio (which among many other business activities,
operates a mobile network) to enter the agri-tech sector and use
AI-powered trading platforms for farmers to consolidate its power, and
many such farmers boycotted the operator by transferring their mobile
service to a competitor.[^10Daly_41]

These examples demonstrate that contestations over AI already occur in
people’s everyday lives, and provoke localized action, including in the
form of protest, which can lead to law and policy change. These everyday
encounters with AI and its politics bring AI ethics (back) from distant
policymakers and political and corporate elites to individuals and
communities, recognizing their/our agency in negotiating and resisting
technology applications. These contestations and resistances can address
the enforceability gap critiqued by Wagner’s ‘ethics washing’ by
provoking action and change to curb uses of AI on a grounded, local
level, compared to the lofty and at times elitist AI ethics initiatives,
which often lack ‘bite’ and tend not to prohibit or severely restrict
certain AI uses and applications.

## Facial Recognition as Everyday AI 

Here I want to focus on the application of AI in the form of facial
recognition, and the everyday encounters people have had with it in
different parts of the world that in some cases have given rise to
everyday AI law and ethics. I concentrate on the U.K. experience of
facial recognition, as it is the geographical location with which I am
most familiar, and one in which we have experienced protest, policy, and
legal events relating to everyday facial recognition use, as well as
differing approaches in different parts of the U.K. to facial
recognition use, which can be juxtaposed with the ‘pro-innovation’ and
neo-imperialist high-level U.K. AI policy.

Facial recognition is a technology which identifies an individual from a
digital image, usually by comparing the features of that person’s face
to stored biometric images of faces in a database. Facial recognition
can be ‘live’ when this image capture and analysis is done in real time,
such as by a ‘smart’ CCTV camera in a public place, using AI.
Controversies have surrounded facial recognition for its inaccuracies,
especially in identifying women compared to men and people of color
compared to white people, with ‘darker-skinned females the most
misclassified group.’[^10Daly_42] Furthermore, the conditions in which facial
recognition technologies are being researched, developed, and trialed
are proving controversial: such as Chinese facial recognition products
used against Uyghurs and other ethnic minorities in Xinjiang/East
Turkestan;[^10Daly_43] and Clearview AI in the west which has scraped photos
from social media without users’ knowledge or permission, and whose
product is used by law enforcement in the U.S. and possibly Europe.[^10Daly_44]
Recently, these scraping processes by Clearview have attracted data
protection infringement decisions and fines in the E.U., U.K., and
Australia.[^10Daly_45]

Facial recognition has been implemented in a wide variety of social,
political, and economic contexts throughout the world, in both
authoritarian regimes and (supposed) liberal democracies. Accordingly,
it is becoming an everyday AI technology, encountered by the general
public as they go about their business, especially in public places.
Importantly, these everyday encounters with facial recognition have led
to processes of negotiation and outright resistance in some cases from
the general public. Facial recognition has been an object for social
movement mobilizations, either specifically against the use of this
surveillance technology, or as part of broader protests. Facial
recognition has also seen the mobilisation of everyday law against it,
and led to questions for how state law addresses it.

Facial recognition and CCTV cameras have been the site of protest and
actual destruction in various locations globally. During protests in
Iran in 2019 against government increases to petrol prices,[^10Daly_46] footage
emerged of protestors disabling and destroying CCTV cameras in different
locations in the country, including Shiraz and Tehran.[^10Daly_47] In more
recent protests in the Khuzestan province in 2021, there is also footage
which appears to show similar attacks on CCTV cameras.[^10Daly_48] There is an
extensive surveillance infrastructure in Iran and in particular since
the 2019 protests, after which, according to Akbari, ‘CCTV cameras
became compulsory in cafes, universities, and even kindergartens.
Traffic control cameras mushroomed in big cities,’ with ‘the government
actively us\[ing\] CCTV/traffic cameras’ footage in tackling political
dissent.’[^10Daly_49]

Also in 2019, suspected facial recognition CCTV cameras were the target
of protestors against the extradition bill and national security law in
Hong Kong, where a ‘lack of trust in technology persists’.[^10Daly_50] Not only
did were ‘face masks, umbrellas and lasers … routinely used by
demonstrators to blind CCTV cameras … thereby render\[ing\] facial
recognition ineffective’,[^10Daly_51] protestors: 'also took down new ‘smart’ lampposts, where their full technological capabilities have not been disclosed, installed by the Government during a protest against surveillance and increasing prevalence of facial recognition technologies.[^10Daly_52]

Protestors not only took down the lampposts but also ‘dissected’ them by
opening up their ‘black boxes’ to see exactly what components and
equipment was inside, including whether facial recognition equipment was
contained within, as the Hong Kong government had claimed that the
lampposts merely monitored air quality and traffic.[^10Daly_53] Some smart
lampposts did have cameras inside them and while it seems that these
cameras did not have facial recognition capacity, independent experts
considered that it would not be difficult to modify the cameras to
include such capabilities.[^10Daly_54] In any event, the Hong Kong authorities
decided not to activate certain features of the smart lampposts due to
privacy concerns.

In both the Iranian and Hong Kong examples, the possibility or reality
of facial recognition technologies in public places has prompted
protests and mobilizations, which can be conceptualized as part of
broader movements responding to material circumstances and against state
power. However, significant in both movements is the popular suspicion
and physical targeting of (possible) facial recognition CCTV, which
demonstrate forms of citizen resistance against aspects of the digital
data (political) economy. In the case of Hong Kong, this contributed to
the Hong Kong authorities deciding not to implement certain aspects of
the smart lampposts, which in the context of the National Security Law
was a notable and rare positive response to the protestors’ concerns,
and also demonstrates government responsiveness to citizen concerns in
the general context of top-down smart city initiatives such as that of
Hong Kong.[^10Daly_55]

## Facial Recognition, Everyday AI Law and Ethics in the UK

Facial recognition as everyday AI, and contestations around it, have
been prominent in the U.K., and mobilization against facial recognition
has resulted in litigation and policy change, and divergence between the
approach in different parts of the U.K. Live facial recognition
technology has been used in different parts of the U.K. to police public
places, to mounting levels of controversy and legal challenge. For these
reasons, I consider it an interesting case study of everyday AI ethics
(and law), and how activities from individuals and communities at a more
localized level in encountering, negotiating and resisting AI can be
impactful for governing AI more generally. Furthermore, the differences
in approach to facial recognition within the U.K. also demonstrate the
importance of looking at the local level as well as the national,
continental, and international. As mentioned above, the U.K. has a
pro-innovation techno-solutionist approach to AI at the ‘high’ level,
but the ‘dark side’ of AI and democratic contestations around it are
only clear if we look at these more localized encounters between facial
recognition and the general public. Contestations around facial
recognition resulting in law and policy change can also be seen in the
U.S., where some municipalities have prohibited police use of facial
recognition, including San Francisco, which was the first to do so.[^10Daly_56]
Two states, Virginia and Vermont, have also banned police use of facial
recognition throughout their territory.[^10Daly_57] Local-level mobilization
against problematic uses of AI such as live facial recognition can lead
to prohibitions, and in a snowballing effect can circulate to inspire
prohibitions elsewhere, forming bottom-up and more critical norms around
AI in distinction to the top-down but often toothless AI ethics
initiatives.

There is a recent history of proposals to use and actual uses of facial
recognition technology, especially by the police and law enforcement, in
controversial contexts within the U.K., even in Scotland, which more
recently introduced a moratorium on these uses. For example, the
Scottish Professional Football League (SPFL) intended to introduce
facial recognition technology in Scottish (soccer) football stadiums as
far back as 2016, in a context of heightened surveillance of football
fans using cameras and worsening relationships between fans and the
police.[^10Daly_58] Various supporters’ groups spoke out against the plans,
including by unveiling anti-facial recognition banners at matches.[^10Daly_59]
Police Scotland also signalled that they wanted to use live facial
recognition in their broader activities, not just vis-à-vis football
fans, and Glasgow, Scotland’s largest city, bought facial
recognition–enabled cameras for the city center in 2015, but these have
not been used due to privacy and human rights concerns.[^10Daly_60] In both
cases, pressure and concern from those against whom the technology would
be used caused public authorities to reconsider and refrain from using
facial recognition, although in the latter case this also involves a
waste of public money in buying technology that has never been used.
This is ironic given the ways in which the police have been encouraged
to turn to private tech providers such as facial recognition providers
as a supposed cost-cutting exercise in the context of austerity and
privatization.[^10Daly_61]

During 2020, the Scottish Parliament’s Justice Sub-Committee considered
police use of facial recognition technology. A consultation process was
held to which I along with various other academics, and civil society
groups contributed, most of us contesting the use of facial recognition
by police, and pointing to discriminatory aspects of it and prohibitions
in other places especially US cities. The Committee concluded that there
was ‘no justifiable basis for Police Scotland to invest in this
technology’, principally due to the gender and racial discrimination the
technology implicates.[^10Daly_62] Since then, there has been a moratorium on
the use of live facial recognition technology by police in Scotland.

This contrasts with the approach taken in England and Wales - which are
the same jurisdiction, Scotland and Northern Ireland each being the
other two jurisdictions which make up the UK. According to Big Brother
Watch:

> Police forces in the UK have rolled out automatic facial recognition
> at a pace unlike any other democratic nation in the world.
> Leicestershire Police, South Wales Police and the Metropolitan Police
> have deployed this technology at shopping centres, festivals, sports
> events, concerts, community events – and even a peaceful
> demonstration. One police force even used the surveillance tool to
> keep innocent people with mental health issues away from a public
> event.[^10Daly_63]

In London, the Metropolitan Police have used facial recognition at
events and in areas with large Black and Minority Ethnic (BAME)
populations, such as in Stratford, East London, and at the Notting Hill
Carnival in 2016 and 2017, despite the inaccuracies facial recognition
produces for Black people. This is also in spite of the already strained
relationship between the Met Police and Black communities.[^10Daly_64] Specific
surveillance and data-gathering activities have impacted Black
communities disproportionately, including the gathering of data for the
Met Police’s controversial ‘Gangs Matrix’ database on individuals
suspected of gang activity, a majority of whom were Black.[^10Daly_65] The U.K.
data protection authority, the Information Commissioner’s Office (ICO),
found that the matrix was not compliant with data protection law, with
the Met Police ordered not to destroy it but to bring it in line with
these norms. The matrix remains controversial and at the time of writing
is subject to another legal challenge led by civil liberties and human
rights NGO Liberty, this time on the grounds of infringing racial
discrimination law as well as human rights, data protection, and public
law principles.[^10Daly_66]

While in the U.K. facial recognition cameras have not been physically
attacked, unlike in the Iranian and Hong Kong contexts, they have still
provoked a visceral response from at least some members of the public
when used in everyday public places. On understanding that facial
recognition cameras were deployed in public, some individuals have
covered their faces to protect their privacy, with at least one person
being fined by the police for doing so.[^10Daly_67] Football fans in Scotland
were also prompted to unveil banners specifically against the use of
facial recognition in the stadium. These may be seen as part of broader
contestations of the ‘hyper-militarization’ of U.K. police, which
Weitzberg identifies as a trend that includes facial recognition
use.[^10Daly_68] There is limited support for facial recognition among the
public more generally and even scepticism from some parts of the police
themselves. A national survey by the Ada Lovelace Institute of public
attitudes to facial recognition showed that a majority of the public
wanted government restrictions on police use of facial recognition and
opposed commercial use of the technology.[^10Daly_69] Urquhart and Miranda’s
research with frontline U.K. police officers showed also that even the
position of police officers was ‘mainly one of scepticism and disbelief
in the technology.’[^10Daly_70]

Critics of live facial recognition in the U.K. have also mobilized the
law, specifically human rights and data protection law, through
litigation, resulting in ‘the first major successful legal challenge to
police use of automated facial recognition technology anywhere in the
world.’[^10Daly_71] A civil liberties campaigner, Ed Bridges, challenged South
Wales Police’s use of live facial recognition, on the basis that it
breached the right to privacy, data protection law and equality laws. At
first instance, the High Court found that while facial recognition did
interfere with the public’s rights, its use by the South Wales Police
was lawful due to safeguards in the framework governing the use of
facial recognition.[^10Daly_72] However, this decision was overturned on
appeal, with the Court of Appeal finding that the use of live facial
recognition did breach human rights, there were ‘fundamental
deficiencies’ in the governing framework and that the police force had
not ensured that the software used was unbiased on grounds of race and
sex.[^10Daly_73] South Wales Police will not appeal this decision. Yet, as with
the ICO’s aforementioned decision about the Met Police’s Gangs Matrix,
the Court of Appeal did not find facial recognition use per se by the
police to be illegal in public places, just that there were not
appropriate safeguards in place: indeed ‘the decision still affirms the
role of automated facial recognition in modern policing and law
enforcement.’[^10Daly_74]

In light of the above, with live facial recognition use by police in
Scotland effectively banned, yet permitted with some limitations in
England and Wales, there is a ‘North–South Divide’ as Lynch has termed
it, regarding police use of live facial recognition as an everyday AI
application in public places in the UK:

> If you find yourself walking in some parts of London or Wales, for
> example, live facial recognition technology will now be able to scan
> your face without consent and you may even be subject to an
> on-the-spot identity check (particularly if you are a woman or an
> ethnic minority). In Scotland, however, you will not have to worry
> about this—at least for now.[^10Daly_75]  

However, police use of facial recognition is only part of the picture.
There have been other controversial uses of facial recognition in
everyday U.K. life, including in Scotland. During 2021, facial
recognition technology was used at nine schools in North Ayrshire to
facilitate quicker and contactless payment for canteen lunches.[^10Daly_76] The
ICO urged the local authority to take a less intrusive approach to
ensure compliance with necessity and proportionality requirements, and
it seems that the use of facial recognition was suspended shortly
after.[^10Daly_77] While this may have nipped facial recognition in schools in
the bud in Scotland, a supermarket, the Co-op (which is traditionally
considered an ethical retailer) is using live facial recognition in
stores in the south of England for safety and security reasons, although
this is opposed by digital rights group Big Brother Watch, which has led
a \#StopCoopSpying social media campaign.[^10Daly_78] At the time of writing,
the Co-op is the subject of a complaint to the ICO by Big Brother Watch
and digital rights agency AWO.[^10Daly_79]

Everyday encounters and contestations of facial recognition in the UK
demonstrate how the public meets AI in the form of facial recognition in
their quotidian lives, through police deployment in public places, to
its use in schools and supermarkets. The most successful influencing of
policymakers can be seen in the Scottish Parliament’s moratorium on
police use of live facial recognition. Legal challenges and use of the
ICO’s complaints process especially by activists and NGOs have produced
some success in reining in facial recognition but are not outright
victories. The unwillingness of the ICO or courts to find the highly
intrusive use of facial recognition in public places illegal outright
demonstrates only partial success in a bottom-up norm forming, although
this may also reflect a deference on behalf of these bodies towards the
U.K. Parliament which they might find to be the more appropriate body to
impose such a ban. Yet we are still waiting for such action, despite
such calls bolstered recently by Matthew Ryder QC’s Independent Review
of the Governance of Biometric Data in England and Wales, who
recommended that a new legislative framework for all uses of biometric
technologies, and legally binding codes of practice for police and other
users of live facial recognition respectively were needed; until these
are implemented, all live facial recognition use should cease.[^10Daly_80]

Norms developed in localized contexts can circulate more
internationally. Sometimes this is due to circulations of national or
global capital, in the cases of laws and policies developed in
California in the U.S., and increasingly the effect of European Union
law and policy more globally, with the ‘Brussels effect’ of its
governance mechanisms influencing law and policy elsewhere due to the
EU’s status as the world’s largest trading bloc and its active stance in
developing and circulating its law and regulation beyond its
borders.[^10Daly_81] This may also be the case for the EU’s proposed AI Act,
which may follow the GDPR in forming a de facto global norm,[^10Daly_82] and
one which at the moment, as mentioned above, will not prohibit outright
the use of facial recognition, even by law enforcement.

Yet, the Scottish example shows how other forms of norm circulation are
possible which are not in the service of global capital and power where
the local prohibitions on police facial recognition use in other parts
of the world being referenced by the Scottish Parliament Justice
Sub-Committee in its call for a moratorium on police use of live facial
recognition in Scotland. This shows that norms developed locally through
negotiation and contestation of AI uses can also circulate more globally
and influence activities elsewhere, leading potentially to a snowballing
effect of localized AI norms that can be leveraged by social movements,
protests, and legal mobilizations in other geographical contexts.

At a ‘high level’, the U.K. has set out its public research funding
approach to AI and its policy intentions as regards a ‘light touch’
non-binding governance of AI, including facial recognition. This
demonstrates a further cleavage with the EU’s intention to regulate AI.
It can be seen as part of the U.K.’s post-Brexit trajectory, which also
involves a distancing from the E.U.’s data protection regime, and
accords with Ossewaarde and Gulenc’s aforementioned observations of the
U.K.’s AI approach as digitally utopian, technologically solutionist,
and neo-imperial.[^10Daly_83] Contestations over facial recognition use in
practice in individuals’ and communities’ everyday encounters in British
public spaces demonstrate how these logics are perpetuated but also
resisted, especially when facial recognition is used as part of the
U.K.’s hyper-militarized law enforcement targeting BAME communities. In
some cases these mobilizations can lead to litigation (albeit with only
limited success so far) and localized policy change, where the
opportunities present themselves. Researchers, activists, and others in
the U.K. may find limited prospects in influencing the U.K. government
centrally, but this case study of facial recognition shows pressure can
be exerted via litigation. There may be more opportunities in
influencing more localized structures of governance, such as the
devolved administrations in Scotland, Wales, and Northern Ireland, where
there may be more prospect of impact. Currently, these devolved
administrations are governed by political parties which are not the
Conversative party in power in the U.K. Parliament, and there may be a
desire to distinguish their policies from that of the U.K. government
for political reasons (heightened in the Brexit and COVID-19 contexts),
leading to fragmentation and differentiation in policy and
governance.[^10Daly_84] Furthermore, there may be fewer attempts from global
capital, especially Big Tech firms, to influence these administrations
in ways favorable to their interests. Such conditions present
possibilities for localized negotiation and resistance to AI and which
have been realized to some degree in reining in facial recognition, and
which can be juxtaposed to the laissez-faire ‘pro-innovation’ approach
of the U.K. government to AI.

## Conclusion

Practices and applications of AI and AI ethics are occurring right here,
right now throughout the world at local and everyday levels, with
members of the general public encountering the technology in its myriad
forms. These encounters—and negotiations and contestations—are rarely
the focus, however, of AI ethics discussions and initiatives. Through
the case study of facial recognition in the U.K., I have demonstrated
how looking at the local is key to understanding how AI ethics plays
out, is formed, and informed in practice, producing at times law and
policy change with ‘bite’, which serve individuals and communities
rather than state power and capital, a ‘bite’ the ethics-washed
higher-level AI ethics initiatives often lack. Accordingly, we need to
engage more with social movements and everyday law and policy in
localities seeking to build, form, and inform better AI. This is where
real change, which does not necessarily serve political and economic
power, can happen, now.

## Acknowledgements

I would like to thank Xaroula Kerasidou, reviewers Jake Goldenfein and
Nancy Salem, and editor Thao Phan for comments on an earlier version of
this paper.

## Funding disclosure

No specific funding source funded this research, it was conducted during
my salaried employment and outside of those hours. During writing this
paper I hold and have held grants funded by UKRI but they did not
support this paper.

## References

Ada Lovelace Institute. ‘Beyond Face Value: Public Attitudes to Facial
Recognition technology’, 2019,
https://www.adalovelaceinstitute.org/report/beyond-face-value-public-attitudes-to-facial-recognition-technology/.

Akbari, Azadeh. ‘The Threat of Automating Control: Surveillance of
Women’s Clothing in Iran’, in Aleš Završnik and Vasja Badalič (eds)
*Automating Crime Prevention, Surveillance, and Military Operations*,
Cham: Springer, 2021, pp. 183–99.

Beraldo, Davide and Milan, Stefania. ‘From Data Politics to the
Contentious Politics of Data’, *Big Data & Society* 6.2 (2019).

Bietti, Elettra. ‘From Ethics Washing to Ethics Bashing: A View on Tech
Ethics from Within Moral Philosophy’, Proceedings of ACM FAT\*
Conference, 2020, https://ssrn.com/abstract=3513182.

Big Brother Watch. ‘Face Off: The Lawless Growth of Facial Recognition
in UK Policing’, 2018,
http://bigbrotherwatch.org.uk/wp-content/uploads/2018/05/Face-Off-final-digital-1.

Bradford, Anu. *The Brussels Effect: How the European Union Rules the
World*. Oxford: Oxford University Press, 2020.

Buolamwini, Joy and Gebru Timnit. ‘Gender Shades: Intersectional
Accuracy Disparities in Commercial Gender Classification’ in Proceedings
of the 1st Conference on Fairness, Accountability and Transparency, PMLR
81, 2018, 77–91.

Burgess, Jean, Mitchell, Peta, and Highfield, Tim. ‘Automating the
Digital Everyday: An Introduction’, *Media International Australia*,
166.1 (2018): 6–10.

Chubb, Jennifer and Reed, Mark. ‘The Politics of Research Impact:
Academic Perceptions of the Implications for Research Funding,
Motivation and Quality’, *British Politics* 13 (2018): 295–311.

Daly, Angela. ‘Algorithmic Oppression with Chinese Characteristics: AI
Against Xinjiang's Uyghurs’ in *Artificial Intelligence: Human Rights,
Social Justice and Development: Global Information Society Watch 2019
Report,* 2019, pp. 108–112.

Daly, Angela, Devitt, S. Kate and Mann, Monique. ‘AI Ethics Needs Good
Data’ in Pieter Verdegem (ed) *AI for Everyone? Critical Perspectives*,
London: University of Westminster Press, 2021, pp. 103–22.

Daly, Angela, Hagendorff, Thilo, Hui, Li, Mann, Monique, Marda, Vidushi,
Wagner, Ben, and Wang, Wayne Wei. ‘AI, Governance and Ethics: Global
Perspectives’ in Hans Micklitz, Oreste Pollicino, Amnon Reichman, Andrea
Simoncini, Giovanni Sartor and Giovanni De Gregorio (eds)
*Constitutional Challenges in the Algorithmic Society*, Cambridge:
Cambridge University Press, 2022, pp. 182–201.

Daly, Angela. ‘Neo-Liberal Business-As-Usual or Post-Surveillance
Capitalism with European Characteristics? The EU’s General Data
Protection Regulation in a Multi-Polar Internet’ in Rolien Hoyng and
Gladys Pak Lei Chong (eds), *Critiquing Communication Innovation: New
Media in a Multipolar World*, East Lansing: Michigan State University
Press, 2022 (forthcoming).

Davies, Pascale. ‘UK Schools Suspend Use of Controversial Facial
Recognition Technology’, *Euronews*, 25 October 2021,
https://www.euronews.com/next/2021/10/18/schools-in-scotland-start-using-facial-recognition-on-children-paying-for-lunch.

Dearden, Lizzie. ‘Police Stop People for Covering Their Faces From
Facial Recognition Camera then Fine Man £90 After he Protested’, *The
Independent*, 31 January 2019,
https://www.independent.co.uk/news/uk/crime/facial-recognition-cameras-technology-london-trial-met-police-face-cover-man-fined-a8756936.html.

Edwards, Rosalind. ‘Why Do Academics Do Unfunded Research? Resistance,
Compliance and Identity in the UK Neo-liberal University’, *Studies in
Higher Education* 47.4 (2022): 904–14.

Elliott, Ian, Bottom, Karin, Carmichael, Paul, Liddle, Joyce, Martin,
Steve, and Pyper, Robert. ‘The Fragmentation of Public Administration:
Differentiated and Decentered Governance in the (dis)United Kingdom’,
*Public Administration* 100.1 (2022): 98–115.

Ewick, Patricia and Silbey, Susan. *The Common Place of Law: Stories
from Everyday Life,* Chicago: University of Chicago Press, 1998.

Feathers, Todd. ‘Facial Recognition Is Racist. Why Aren’t More Cities
Banning It?’, *Vice*, 25 May 2021,
https://www.vice.com/en/article/4avx3m/facial-recognition-is-racist-why-arent-more-cities-banning-it.

Finn, Mike. *British Universities in the Brexit Moment : Political,
Economic and Cultural Implications*. Bingley: Emerald Publishing, 2018.

Gleeson, Sean. ‘How Smart are Hong Kong’s Lampposts?’, *AFP Fact Check*,
4 September 2019,
https://factcheck.afp.com/how-smart-are-hong-kongs-lampposts.

Hagendorff, Thilo. ‘The Ethics of AI Ethics: An Evaluation of
Guidelines’, *Minds & Machines* 30 (2021): 99–120.

Hagendorff, Thilo. ‘Blind Spots in AI ethics’, *AI Ethics* (2021)*.*

Hamed, Assiah. ‘Co-op Defends Facial Recognition Cameras in Bristol
Stores Amid Claims of ‘Orwellian’ Surveillance’, *Bristol Post*, 9
December 2021,
https://www.bristolpost.co.uk/whats-on/shopping/co-op-defends-facial-recognition-6302476
(accessed 1 February 2022).

Hamilton-Smith, Niall, McBride, Maureen, and Atkinson, Colin. ‘Lights,
Camera, Provocation? Exploring Experiences of Surveillance in the
Policing of Scottish Football’, *Policing and Society* 31.2 (2021):
179–94.

Heikkilä, Melissa. ‘The Walls are Closing in on Clearview AI’, *MIT
Technology Review*, 24 May 2022,
https://www.technologyreview.com/2022/05/24/1052653/clearview-ai-data-privacy-uk/.

Hunton Andrews Kurth. ‘UK Court of Appeal Finds Automated Facial
Recognition Technology Unlawful in Bridges v South Wales Police’, 12
August 2020,
https://www.huntonprivacyblog.com/2020/08/12/uk-court-of-appeal-finds-automated-facial-recognition-technology-unlawful-in-bridges-v-south-wales-police/.

Jobin, Anna, Ienca, Marcello, and Vayena, Effy. ‘The Global Landscape of
AI Ethics Guidelines’, *Nature Machine Intelligence* 1 (2019): 389–99.

Kaun, Anne. ‘Suing the Algorithm: the Mundanization of Automated
Decision-making in Public Services Through Litigation’, *Information,
Communication & Society* (2021).

Kazansky, Becky, Torres, Guillen, van der Velden, Lonneke, Wissenbach,
Kersti, and Milan, Stefania. ‘Data for the Social Good: Towards a
Data-Activist Research Agenda’ in Angela Daly, S. Kate Devitt and
Monique Mann (eds) *Good Data*, Amsterdam: Institute of Network
Cultures, 2019, pp. 244–59.

Kazansky, Becky. ‘“It Depends on Your Threat Model”: The Anticipatory
Dimensions of Resistance to Data-driven Surveillance’, *Big Data &
Society* 8.1 (2021).

Kazim, Emre, Almeida, Denise, Kingsman, Nigel, Kerrigan, Charles,
Koshiyama, Adriano, Lomas, Elizabeth and Hilliard, Airlie. ‘Innovation
and opportunity: review of the UK’s national AI strategy’, *Discover
Artificial Intelligence* 1.14 (2021): 1–10.

Kewalramani, Manoj and Seth, Rohan. ‘Networked Protests & State
Responses: The Case of Hong Kong 2019-2020’, Takshashila Discussion
Document 2020-03, 2020, https://ssrn.com/abstract=3580591.

Lee, Dave. ‘San Francisco is First US City to Ban Facial Recognition’,
*BBC,* 15 May 2019, https://www.bbc.co.uk/news/technology-48276660.

Leung, Kevin and Lee, H.Y. ‘Implementing the Smart City: Who Has a Say?
Some Insights from Hong Kong’, *International Journal of Urban
Sciences*(2021): 1–25.

Long, Lisa and Joseph-Salisbury, Remi. ‘Black Mixed-race Men’s
Perceptions and Experiences of the Police’, *Ethnic and Racial Studies*
42.2 (2019): 198–215.

Lu, Alex Jiahong. ‘Toward Everyday Negotiation and Resistance Under
Data-Driven Surveillance’, *Interactions* 29.2 (2022): 34–8.

Lynch, Euan. ‘The Use of Live Facial Recognition Technology in Scotland:
A New North-South Divide?’, *UK Human Rights Blog*, 25 February 2020,
https://ukhumanrightsblog.com/2020/02/25/the-use-of-live-facial-recognition-technology-in-scotland-a-new-north-south-divide/.

Mann, Monique. ‘Social (In)security and Social (In)justice: Automation
in the Australian Welfare System’ in *Artificial intelligence: Human
rights, social justice and development: Global Information Society Watch
2019 Report,* 2019, pp. 68–72.

McCann, Michael. ‘Law and Social Movements: Contemporary Perspectives’,
*Annual Review of Law and Social Science* 2.1 (2006): 17–38.

McGowran, Leigh. ‘The Issues with the EU’s Draft Regulation on Facial
Recognition AI’, *Silicon Republic*, 17 May 2022,
https://www.siliconrepublic.com/enterprise/the-issues-with-the-eus-draft-regulation-on-facial-recognition-ai.

Mega, Marcello. ‘Cops Fear Gangsters are Evading Law as Glasgow’s Facial
Recognition Cameras Remain Mothballed’, *Daily Record*, 11 August 2020,
https://www.dailyrecord.co.uk/news/scottish-news/scots-cops-fear-gangsters-evading-22499468.

Moule, Richard, Burruss, George, Gifford, Faith, Parry, Megan, and Fox,
Bryanna. ‘Legal Socialization and Subcultural Norms: Examining Linkages
Between Perceptions of Procedural Justice, Legal Cynicism, and the Code
of the Street’, *Journal of Criminal Justice* 61(2019): 26–39.

Nijjar, Jasbinder. ‘Police–school Partnerships and the War on Black
Youth’, *Critical Social Policy*, 41.3 (2021): 491–501.

ÓhÉigeartaigh, Seán, Whittlestone, Jess, Liu, Yang, Zeng, Yi, and Liu,
Zhe. ‘Overcoming Barriers to Cross-cultural Cooperation in AI Ethics and
Governance’, *Philosophy and Technology* 33 (2020): 571–93.

Ossewaarde, Marinus and Gulenc, Erdener. ‘National Varieties of
Artificial Intelligence Discourses: Myth, Utopianism, and Solutionism in
West European Policy Expectations’, *Computer* 53.11 (2020): 53–61.

Parida, Tulsi and Ashok, Aparna. ‘Consolidating Power in the Name of
Progress: Techno-solutionism and Farmer Protests in India’ in Frederike
Kaltheuner (ed), *Fake AI,* Manchester: Meatspace Press, 2021, pp.
161–9.

Phan, Thao, Goldenfein, Jake, Mann, Monique, and Kuch, Declan.
‘Economies of Virtue: The Circulation of “Ethics” in Big Tech’, *Science
as Culture*, 31.1 (2022): 121–35.

Pink, Sarah, Ruckenstein, Minna, Berg, Martin and Lupton, Deborah.
‘Everyday Automation: Setting a Research Agenda’ in Pink, Sarah, Berg,
Martin, Lupton, Deborah, and Ruckenstein, Minna (eds) *Everyday
Automation: Experiencing and Anticipating Emerging Technologies*,
London: Routledge, 2022, pp. 1–20.

Rezende, Isadora Neroni. ‘Facial Recognition in Police Hands: Assessing
the ‘Clearview Case’ From a European Perspective’, *New Journal of
European Criminal Law* 11.3 (2020): 375–89.

Ruthven, Graham. ‘The Criminalization of Scottish Soccer Fans’, *Vice*,
23 February 2016,
https://www.vice.com/en/article/9apyad/the-criminalization-of-scottish-soccer-fans.

Ryder, Matthew. ‘Independent Legal Review of the Governance of Biometric
Data in England and Wales (‘The Ryder Review’)’, Ada Lovelace Institute
(2022),
https://www.adalovelaceinstitute.org/wp-content/uploads/2022/06/The-Ryder-Review-Independent-legal-review-of-the-governance-of-biometric-data-in-England-and-Wales-Ada-Lovelace-Institute-June-2022.pdf.

Sarat, Austin and Kearns, Thomas (eds) *Law in Everyday Life*, Ann
Arbor: University of Michigan Press, 1995.

Scottish Parliament Justice Sub-Committee on Policing. ‘Facial
Recognition: How Policing in Scotland Makes Use of This Technology’, SP
Paper 678, 1st Report (Session 5), 2020,
https://sp-bpr-en-prod-cdnep.azureedge.net/published/JSP/2020/2/11/Facial-recognition--how-policing-in-Scotland-makes-use-of-this-technology/JSPS0520R01.pdf.

Shahi, Afshin and Abdoh-Tabrizi, Ehsan. ‘Iran’s 2019-2020
Demonstrations: The Changing Dynamics of Political Protests in Iran’,
*Asian Affairs* 51.1 (2020): 1–41.

Sharma, Suneet. ‘Case Law: R (Bridges) v Chief Constable of South Wales
Police: The Use of Facial Recognition Software by the Police is Lawful’,
*Inforrm blog*, 6 September 2019,
https://inforrm.org/2019/09/06/case-law-r-bridges-v-chief-constable-of-south-wales-police-the-use-of-facial-recognition-software-by-the-police-is-lawful-suneet-sharma/.

U.K. Government. ‘National AI Strategy’, 21 September 2021,
https://www.gov.uk/government/publications/national-ai-strategy.

U.K. Government. ‘Establishing a Pro-innovation Approach to Regulating
AI’, 20 July 2022,
https://www.gov.uk/government/publications/establishing-a-pro-innovation-approach-to-regulating-ai.

UKRI Economic and Social Research Council. ‘Defining Impact’,
https://www.ukri.org/councils/esrc/impact-toolkit-for-economic-and-social-sciences/defining-impact/.

UNESCO. ‘UNESCO Member States Adopt the First Ever Global Agreement on
the Ethics of Artificial Intelligence’, 25 November 2021,
https://en.unesco.org/news/unesco-member-states-adopt-first-ever-global-agreement-ethics-artificial-intelligence.

Urquhart, Lachlan and Miranda, Diana. ‘Policing faces: the present and
future of intelligent facial surveillance’, *Information &
Communications Technology Law*, 31:2 (2022): 194-219.

Veale, Michael and Zuiderveen Borgesius, Frederik. ‘Demystifying the
Draft EU Artificial Intelligence Act—Analysing the Good, the Bad, and
the Unclear Elements of the Proposed Approach’, *Computer Law Review
International* 22.4 (2021): 97–112.

Wagner, Ben. ‘Ethics as an Escape from Regulation: From ‘Ethics-Washing’
to Ethics-Shopping?’ in Bayamlioglu, Emre, Baraliuc, Irina, Janssens,
Liisa, and Hildebrandt, Mireille (eds) *Being Profiled: Cogitas Ergo
Sum: 10 Years of Profiling the European Citizen*, Amsterdam: Amsterdam
University Press, 2018, pp. 84–9.

Weale, Sally. ‘ICO to Step in After Schools Use Facial Recognition to
Speed Up Lunch Queue’, *Guardian*, 18 October 2021,
https://www.theguardian.com/education/2021/oct/18/privacy-fears-as-schools-use-facial-recognition-to-speed-up-lunch-queue-ayrshire-technology-payments-uk.

Weitzberg, Keren. ‘A Very British Problem: The Evolution of Britain’s
Militarised Policing Industrial Complex’, Report for Campaign Against
the Arms Trade and Netpol, 2022,
https://caat.org.uk/app/uploads/2022/08/A-Very-British-Problem-WEB.pdf.

White, Nadine. ‘Met Police Faces Legal Action Over ‘Racist’ Gangs Matrix
Database’, *Independent*, 1 February 2022,
https://www.independent.co.uk/news/uk/home-news/met-police-gangs-matrix-database-b2004293.html.

Willim, Robert. ‘Imperfect Imaginaries: Digitisation, Mundanisation, and
the Ungraspable’ in Gertraud Koch (ed) *Digitisation: Theories and
Concepts for Empirical Cultural Research*, Abingdon: Routledge, 2017,
pp. 53–77.

Wong, Janis. ‘Protests Decentralised: How Technology Enabled Civil
Disobedience by Hong Kong Anti-extradition Bill Protesters’, *LawArXiv*,
2020, https://osf.io/preprints/lawarxiv/efvwn/.

Wong, Pak-Hang. ‘Cultural Differences as Excuses? Human Rights and
Cultural Values in Global Ethics and Governance of AI’, *Philosophy and
Technology* 33 (2020): 705–15.

Zalnieriute, Monika. ‘Burning Bridges: The Automated Facial Recognition
Technology and Public Space Surveillance in the Modern State’, *Science
and Technology Law Review*, 22.2 (2021): 284–307.

[^10Daly_1]: Pak-Hang Wong, ‘Cultural Differences as Excuses? Human Rights and
    Cultural Values in Global Ethics and Governance of AI’, *Philosophy
    and Technology* 33 (2020): 705–715.

[^10Daly_2]: Seán ÓhÉigeartaigh, Jess Whittlestone, Yang Liu, Yi Zeng and Zhe
    Liu, ‘Overcoming Barriers to Cross-cultural Cooperation in AI Ethics
    and Governance’, *Philosophy and Technology* 33 (2020): 571–593.

[^10Daly_3]: I thank Xaroula Kerasidou for this point.

[^10Daly_4]: UKRI Economic and Social Research Council, ‘Defining Impact’,
    https://www.ukri.org/councils/esrc/impact-toolkit-for-economic-and-social-sciences/defining-impact/.

[^10Daly_5]: Jennifer Chubb and Mark Reed, ‘The Politics of Research Impact:
    Academic Perceptions of the Implications for Research Funding,
    Motivation and Quality’, *British Politics* 13 (2018): 302.

[^10Daly_6]: Thao Phan, Jake Goldenfein, Monique Mann and Declan Kuch,
    ‘Economies of Virtue: The Circulation of ‘Ethics’ in Big Tech’,
    *Science as Culture* 31.1 (2022): 121–35.

[^10Daly_7]: Mike Finn, *British Universities in the Brexit Moment: Political,
    Economic and Cultural Implications*, Bingley: Emerald Publishing,
    2018, p. 97.

[^10Daly_8]: See Cath & Keyes, Pink, & Richardson in this collection.

[^10Daly_9]: Rosalind Edwards, ‘Why do Academics do Unfunded Research?
    Resistance, Compliance and Identity in the UK Neo-liberal
    University’, *Studies in Higher Education*, 47.4 (2022): 912.

[^10Daly_10]: Edwards, ‘Why do academics do unfunded research?’, 912.

[^10Daly_11]: U.K. Government, ‘National AI Strategy’, 21 September 2021,
    https://www.gov.uk/government/publications/national-ai-strategy;
    U.K. Government, ‘Establishing a pro-innovation approach to
    regulating AI’, 20 July 2022,
    https://www.gov.uk/government/publications/establishing-a-pro-innovation-approach-to-regulating-ai.

[^10Daly_12]: Emre Kazim, Denise Almeida, Nigel Kingsman, Charles Kerrigan,
    Adriano Koshiyama, Elizabeth Lomas and Airlie Hilliard, ‘Innovation
    and Opportunity: Review of the UK’s National AI Strategy’, *Discover
    Artificial Intelligence* 1.14 (2021): 1–10.

[^10Daly_13]: Marinus Ossewaarde and Erdener Gulenc, ‘National Varieties of
    Artificial Intelligence Discourses: Myth, Utopianism, and
    Solutionism in West European Policy Expectations’, *Computer* 53.11
    (2020): 53–61.

[^10Daly_14]: Ossewaarde and Gulenc, ‘National Varieties of Artificial
    Intelligence Discourses’.

[^10Daly_15]: Thilo Hagendorff, ‘Blind Spots in AI ethics’, *AI Ethics* (2021).

[^10Daly_16]: Thilo Hagendorff, ‘The Ethics of AI Ethics: An Evaluation of
    Guidelines’, *Minds & Machines* 30 (2020), 99–120, 105.

[^10Daly_17]: Hagendorff, ‘Blind Spots in AI ethics’.

[^10Daly_18]: I thank Jake Goldenfein for this point.

[^10Daly_19]: I thank Jake Goldenfein for this point.

[^10Daly_20]: I thank Jake Goldenfein for this point.

[^10Daly_21]: Jean Burgess, Peta Mitchell and Tim Highfield, ‘Automating the
    Digital Everyday: An Introduction’, *Media International Australia*,
    166.1 (2018): 6-10, 6.

[^10Daly_22]: Pink, Ruckenstein, Berg and Lupton, ‘Everyday Automation: Setting
    a Research Agenda’.

[^10Daly_23]: See for example, Anna Jobin, Marcello Ienca and Effy Vayena, ‘The
    Global Landscape of AI Ethics Guidelines’, *Nature Machine
    Intelligence* 1 (2019): 389–99.

[^10Daly_24]: Pink, Ruckenstein, Berg and Lupton, ‘Everyday Automation: Setting
    a Research Agenda’, 8.

[^10Daly_25]: Alex Jiahong Lu, ‘Toward Everyday Negotiation and Resistance
    Under Data-Driven Surveillance’, *Interactions* 29.2 (2022).

[^10Daly_26]: Ben Wagner, ‘Ethics as an Escape from Regulation: From
    ‘Ethics-Washing’ to Ethics-Shopping?’, in Emre Bayamlioglu, Irina
    Baraliuc, Liisa Janssens and Mireille Hildebrandt (eds) *Being
    Profiled: Cogitas Ergo Sum: 10 Years of Profiling the European
    Citizen*, Amsterdam: Amsterdam University Press, 2018, pp. 84–9.

[^10Daly_27]: Elettra Bietti, ‘From Ethics Washing to Ethics Bashing: A View on
    Tech Ethics from Within Moral Philosophy’, Proceedings of ACM FAT\*
    Conference, 2020, https://ssrn.com/abstract=3513182.

[^10Daly_28]: Angela Daly, S. Kate Devitt and Monique Mann, ‘AI Ethics Needs
    Good Data’, in Pieter Verdegem (ed) *AI for Everyone? Critical
    Perspectives*, London: University of Westminster Press, 2021.

[^10Daly_29]: Angela Daly, Thilo Hagendorff, Li Hui, Monique Mann, Vidushi
    Marda, Ben Wagner and Wayne Wei Wang, ‘AI, Governance and Ethics:
    Global Perspectives’ in Hans Micklitz, Oreste Pollicino, Amnon
    Reichman, Andrea Simoncini, Giovanni Sartor and Giovanni De Gregorio
    (eds) *Constitutional Challenges in the Algorithmic Society*,
    Cambridge: Cambridge University Press, 2022.

[^10Daly_30]: UNESCO, ‘UNESCO member states adopt the first ever global
    agreement on the Ethics of Artificial Intelligence’, 25 November
    2021,
    *https://en.unesco.org/news/unesco-member-states-adopt-first-ever-global-agreement-ethics-artificial-intelligence*.

[^10Daly_31]: See for example, Michael Veale and Frederik Zuiderveen Borgesius,
    ‘Demystifying the Draft EU Artificial Intelligence Act—Analysing the
    Good, the Bad, and the Unclear Elements of the Proposed Approach’,
    *Computer Law Review International* 22.4 (2021): 97–112.

[^10Daly_32]: Leigh McGowran, ‘The Issues with the EU’s Draft Regulation on
    Facial Recognition AI’, *Silicon Republic*, 17 May 2022,
    https://www.siliconrepublic.com/enterprise/the-issues-with-the-eus-draft-regulation-on-facial-recognition-ai.

[^10Daly_33]: See for example, Patricia Ewick and Susan Silbey, *The Common
    Place of Law: Stories from Everyday Life,* Chicago: University of
    Chicago Press, 1998; Richard Moule, George Burruss, Faith Gifford,
    Megan Parry and Bryanna Fox, ‘Legal Socialization and Subcultural
    Norms: Examining Linkages Between Perceptions of Procedural Justice,
    Legal Cynicism, and the Code of the Street’, *Journal of Criminal
    Justice* 61 (2019): 26–39.

[^10Daly_34]: Austin Sarat and Thomas Kearns (eds) *Law in Everyday Life*. Ann
    Arbor: University of Michigan Press, 1995, p. 9.

[^10Daly_35]: Michael McCann, ‘Law and Social Movements: Contemporary
    Perspectives’, *Annual Review of Law and Social Science* 2.1 (2006):
    17–38.

[^10Daly_36]: Becky Kazansky, Guillen Torres, Lonneke van der Velden, Kersti
    Wissenbach, and Stefania Milan, ‘Data for the Social Good: Towards a
    Data-Activist Research Agenda’, in Angela Daly, S. Kate Devitt and
    Monique Mann (eds), *Good Data*, Amsterdam: Institute of Network
    Cultures, 2019, 246.

[^10Daly_37]: Davide Beraldo and Stefania Milan, ‘From Data Politics to the
    Contentious Politics of Data’, *Big Data & Society’* 6.2 (2019): 2.

[^10Daly_38]: Becky Kazansky, ‘“It Depends on your Threat Model”: The
    Anticipatory Dimensions of Resistance to Data-driven Surveillance’,
    *Big Data & Society* 8.1 (2021): 1. See also Lu, ‘Toward Everyday
    Negotiation and Resistance Under Data-Driven Surveillance’.

[^10Daly_39]: Anne Kaun, ‘Suing the Algorithm: The Mundanization of Automated
    Decision-making in Public Services Through Litigation*’,
    Information, Communication & Society* (2021); Robert Willim,
    ‘Imperfect imaginaries: Digitisation, mundanisation, and the
    ungraspable’ in Gertraud Koch (ed), *Digitisation: Theories and
    Concepts for Empirical Cultural Research*, Abingdon: Routledge,
    2017.

[^10Daly_40]: Monique Mann, ‘Social (In)security and Social (In)justice:
    Automation in the Australian Welfare System’ in *Artificial
    Intelligence: Human Rights, Social Justice and Development: Global
    Information Society Watch 2019 Report,* 2019*,* pp. 68–72.

[^10Daly_41]: Tulsi Parida and Aparna Ashok, ‘Consolidating Power in the Name
    of Progress: Techno-solutionism and Farmer Protests in India’ in
    Frederike Kaltheuner (ed), *Fake AI*, Manchester: Meatspace Press,
    2021, pp. 161–9.

[^10Daly_42]: Joy Buolamwini and Timnit Gebru, ‘Gender Shades: Intersectional
    Accuracy Disparities in Commercial Gender Classification’ in
    Proceedings of the 1st Conference on Fairness, Accountability and
    Transparency, PMLR 81, 2018, 1.

[^10Daly_43]: Angela Daly, ‘Algorithmic Oppression with Chinese
    Characteristics: AI Against Xinjiang's Uyghurs’ in *Artificial
    Intelligence: Human Rights, Social Justice and Development: Global
    Information Society Watch 2019 Report,* 2019, pp. 108–12.

[^10Daly_44]: Isadora Neroni Rezende, ‘Facial Recognition in Police Hands:
    Assessing the ‘Clearview case’ from a European Perspective’, *New
    Journal of European Criminal Law* 11.3 (2020): 375–89.

[^10Daly_45]: Melissa Heikkilä, ‘The Walls are Closing in on Clearview AI’,
    *MIT Technology Review*, 24 May 2022,
    https://www.technologyreview.com/2022/05/24/1052653/clearview-ai-data-privacy-uk/.

[^10Daly_46]: Afshin Shahi and Ehsan Abdoh-Tabrizi, ‘Iran’s 2019–2020
    Demonstrations: The Changing Dynamics of Political Protests in
    Iran’, *Asian Affairs* 51.1 (2020): 1–41.

[^10Daly_47]: See for example, @DrParchizadeh, ‘Protesters in Tehran sabotage
    the police CCTV so that they can’t be identified, arrested and
    killed by the regime.
    [\#IranProtests](https://twitter.com/hashtag/IranProtests?src=hashtag_click)’,
    Twitter post, 16 November 2019, 4:25PM,
    https://twitter.com/DrParchizadeh/status/1195739605460496385.

[^10Daly_48]: @javidirani30, ‘Last night, Monday, July 19th, Ahwazi youths in
    Alavi alley disabled CCTV cameras \#Khuzestan \#IranProtests’,
    Twitter post, 20 July 2021, 8:45AM,
    https://twitter.com/javidirani30/status/1417390100720279569*.*

[^10Daly_49]: Azadeh Akbari, ‘The Threat of Automating Control: Surveillance of
    Women’s Clothing in Iran’, in Aleš Završnik and Vasja Badalič (eds)
    *Automating Crime Prevention, Surveillance, and Military
    Operations*, Cham: Springer, 2021, 186.

[^10Daly_50]: Janis Wong, ‘Protests Decentralised: How Technology Enabled Civil
    Disobedience by Hong Kong Anti-extradition Bill Protesters’,
    *LawArXiv*, 2020, https://osf.io/preprints/lawarxiv/efvwn/.

[^10Daly_51]: Manoj Kewalramani and Rohan Seth, ‘Networked Protests & State
    Responses: The Case of Hong Kong 2019–2020’, Takshashila Discussion
    Document 2020-03, 2020, https://ssrn.com/abstract=3580591.

[^10Daly_52]: Wong, ‘Protests Decentralised’, 6.

[^10Daly_53]: ‘Hong Kong: Anti-surveillance Protestors Tear Down ‘Smart’
    Lamp-post’, *Guardian*, 26 August 2019,
    https://www.theguardian.com/world/video/2019/aug/26/hong-kong-anti-surveillance-protesters-tear-down-smart-lamp-post-video.

[^10Daly_54]: Sean Gleeson, ‘How Smart are Hong Kong’s Lampposts?’, *AFP Fact
    Check*, 4 September 2019,
    <https://factcheck.afp.com/how-smart-are-hong-kongs-lampposts>.

[^10Daly_55]: Kevin Leung and H.Y. Lee, ‘Implementing the Smart City: Who Has a
    Say? Some Insights from Hong Kong’, *International Journal of Urban
    Sciences*, 2021, 1–25.

[^10Daly_56]: Dave Lee, ‘San Francisco is First US City to Ban Facial
    Recognition’, *BBC,* 15 May 2019,
    https://www.bbc.co.uk/news/technology-48276660.

[^10Daly_57]: Todd Feathers, ‘Facial Recognition Is Racist. Why Aren’t More
    Cities Banning It?’, *Vice*, 25 May 2021,
    https://www.vice.com/en/article/4avx3m/facial-recognition-is-racist-why-arent-more-cities-banning-it.

[^10Daly_58]: Niall Hamilton-Smith, Maureen McBride and Colin Atkinson,
    ‘Lights, Camera, Provocation? Exploring Experiences of Surveillance
    in the Policing of Scottish Football’, *Policing and Society* 31.2
    (2021): 179–94.

[^10Daly_59]: Graham Ruthven, ‘The Criminalization of Scottish Soccer Fans’,
    *Vice*, 23 February 2016,
    https://www.vice.com/en/article/9apyad/the-criminalization-of-scottish-soccer-fans.

[^10Daly_60]: Marcello Mega, ‘Cops Fear Gangsters are Evading Law as Glasgow’s
    Facial Recognition Cameras Remain Mothballed’, *Daily Record*, 11
    August 2020,
    https://www.dailyrecord.co.uk/news/scottish-news/scots-cops-fear-gangsters-evading-22499468.

[^10Daly_61]: Keren Weitzberg, ‘A Very British Problem: The Evolution of
    Britain’s Militarised Policing Industrial Complex’, Report for
    Campaign Against the Arms Trade and Netpol, 2022,
    https://caat.org.uk/app/uploads/2022/08/A-Very-British-Problem-WEB.pdf.

[^10Daly_62]: Scottish Parliament Justice Sub-Committee on Policing, ‘Facial
    Recognition: How Policing in Scotland Makes Use of This Technology’,
    SP Paper 678, 1st Report (Session 5), 2020,
    https://sp-bpr-en-prod-cdnep.azureedge.net/published/JSP/2020/2/11/Facial-recognition--how-policing-in-Scotland-makes-use-of-this-technology/JSPS0520R01.pdf.

[^10Daly_63]: Big Brother Watch, ‘Face Off: The Lawless Growth of Facial
    Recognition in UK Policing’, 2018,
    http://bigbrotherwatch.org.uk/wp-content/uploads/2018/05/Face-Off-final-digital-1.pdf.

[^10Daly_64]: See for example, Lisa Long and Remi Joseph-Salisbury, ‘Black
    Mixed-race Men’s Perceptions and Experiences of the Police’, *Ethnic
    and Racial Studies* 42.2 (2019): 198–215.

[^10Daly_65]: Jasbinder Nijjar, ‘Police–school Partnerships and the War on
    Black Youth’, *Critical Social Policy* 41.3 (2021): 491–501.

[^10Daly_66]: Nadine White, ‘Met Police Faces Legal Action Over ‘Racist’ Gangs
    Matrix Database’, *Independent*, 1 February 2022,
    https://www.independent.co.uk/news/uk/home-news/met-police-gangs-matrix-database-b2004293.html.

[^10Daly_67]: Lizzie Dearden, ‘Police Stop People for Covering their Faces from
    Facial Recognition Camera Then Fine Man £90 After he Protested’,
    *The Independent*, 31 January 2019,
    https://www.independent.co.uk/news/uk/crime/facial-recognition-cameras-technology-london-trial-met-police-face-cover-man-fined-a8756936.html.

[^10Daly_68]: Weitzberg, ‘A Very British Problem’.

[^10Daly_69]: Ada Lovelace Institute, ‘Beyond Face Value: Public Attitudes to
    Facial Recognition Technology’, 2019,
    https://www.adalovelaceinstitute.org/report/beyond-face-value-public-attitudes-to-facial-recognition-technology/.

[^10Daly_70]: Lachlan Urquhart and Diana Miranda, ‘Policing Faces: The Present
    and Future of Intelligent Facial Surveillance’, *Information &
    Communications Technology Law* 31.2 (2022): 194-219, 198.

[^10Daly_71]: Monika Zalnieriute, ‘Burning Bridges: The Automated Facial
    Recognition Technology and Public Space Surveillance in the Modern
    State’, *Science and Technology Law Review*, 22.2 (2021): 284-307,
    287.

[^10Daly_72]: Suneet Sharma, ‘Case Law: R (Bridges) v Chief Constable of South
    Wales Police: The Use of Facial Recognition Software by the Police
    is Lawful’, *Inforrm blog*, 6 September 2019,
    https://inforrm.org/2019/09/06/case-law-r-bridges-v-chief-constable-of-south-wales-police-the-use-of-facial-recognition-software-by-the-police-is-lawful-suneet-sharma/.

[^10Daly_73]: Hunton Andrews Kurth, ‘UK Court of Appeal Finds Automated Facial
    Recognition Technology Unlawful in Bridges v South Wales Police’ 12
    August 2020,
    https://www.huntonprivacyblog.com/2020/08/12/uk-court-of-appeal-finds-automated-facial-recognition-technology-unlawful-in-bridges-v-south-wales-police/;
    see also Urquhart and Miranda, ‘Policing Faces’.

[^10Daly_74]: Zalnieriute, ‘Burning Bridges’.

[^10Daly_75]: Euan Lynch, ‘The Use of Live Facial Recognition Technology in
    Scotland: A New North–South Divide?’, *UK Human Rights blog*, 25
    February 2020,
    https://ukhumanrightsblog.com/2020/02/25/the-use-of-live-facial-recognition-technology-in-scotland-a-new-north-south-divide/.

[^10Daly_76]: Sally Weale, ‘ICO to Step In After Schools Use Facial Recognition
    to Speed Up Lunch Queue’, *Guardian*, 18 October 2021,
    https://www.theguardian.com/education/2021/oct/18/privacy-fears-as-schools-use-facial-recognition-to-speed-up-lunch-queue-ayrshire-technology-payments-uk.

[^10Daly_77]: Pascale Davies, ‘UK Schools Suspend Use of Controversial Facial
    Recognition Technology’, *Euronews*, 25 October 2021,
    https://www.euronews.com/next/2021/10/18/schools-in-scotland-start-using-facial-recognition-on-children-paying-for-lunch.

[^10Daly_78]: Asssiah Hamed, ‘Co-op Defends Facial Recognition Cameras in
    Bristol Stores Amid Claims of ‘Orwellian’Ssurveillance’, *Bristol
    Post*, 9 December 2021,
    https://www.bristolpost.co.uk/whats-on/shopping/co-op-defends-facial-recognition-6302476.

[^10Daly_79]: ‘Southern Co-operative’s Use of Facial Recognition on Customers
    Prompts Legal Complaint’, *Sky News*, 27 July 2022,
    https://news.sky.com/story/co-ops-use-of-facial-recognition-on-customers-prompts-legal-complaint-12659309.

[^10Daly_80]: Matthew Ryder, ‘Independent Legal Review of the Governance of
    Biometric Data in England and Wales (‘The Ryder Review’)’, Ada
    Lovelace Institute, 2022,
    https://www.adalovelaceinstitute.org/wp-content/uploads/2022/06/The-Ryder-Review-Independent-legal-review-of-the-governance-of-biometric-data-in-England-and-Wales-Ada-Lovelace-Institute-June-2022.pdf.

[^10Daly_81]: Anu Bradford, *The Brussels Effect: How the European Union Rules
    the World*. Oxford: Oxford University Press, 2020.

[^10Daly_82]: Angela Daly, ‘Neo-Liberal Business-As-Usual or Post-Surveillance
    Capitalism With European Characteristics? The EU’s General Data
    Protection Regulation in a Multi-Polar Internet’, in Rolien Hoyng
    and Gladys Pak Lei Chong (eds) *Critiquing Communication Innovation:
    New Media in a Multipolar World*, East Lansing: Michigan State
    University Press, 2022 (forthcoming).

[^10Daly_83]: Ossewaarde and Gulenc, ‘National Varieties of Artificial
    Intelligence Discourses’.

[^10Daly_84]: Ian Elliott, Karin Bottom, Paul Carmichael, Joyce Liddle, Steve
    Martin, and Robert Pyper, ‘The Fragmentation of Public
    Administration: Differentiated and Decentered Governance in the
    (dis)United Kingdom’, *Public Administration* 100.1 (2022): 98–115.